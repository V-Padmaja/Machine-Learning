{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V Padmaja 19BAI1107 - LAB 7\n",
    "#### Group (V Padmaja, Sruthi Nayagi, Tejasri BN, AR Arvind) \n",
    "##  a) Binary Classification using Single-layer Perceptron\n",
    "# V9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import MinMaxScaler,MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('Multiclass-dataset.xlsx')\n",
    "df.head()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=df.loc[:,df.columns!='Target Label'].values[:,1:]\n",
    "labels=df.loc[:,'Target Label'].values\n",
    "labels\n",
    "len(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['V1', 'V1', 'V1', 'V1', 'V1', 'V1', 'V1', 'V1', 'V1', 'V1', 'V1',\n",
       "       'V1', 'V1', 'V1', 'V1', 'V1', 'V1', 'V1', 'V1', 'V1', 'V1', 'V1',\n",
       "       'V1', 'V1', 'V1', 'V1', 'V1', 'V1', 'V1', 'V1', 'V1', 'V1', 'V1',\n",
       "       'V1', 'V1', 'V1', 'V1', 'V1', 'V1', 'V1', 'V2', 'V2', 'V2', 'V2',\n",
       "       'V2', 'V2', 'V2', 'V2', 'V2', 'V2', 'V2', 'V2', 'V2', 'V2', 'V2',\n",
       "       'V2', 'V2', 'V2', 'V2', 'V2', 'V2', 'V2', 'V2', 'V2', 'V2', 'V2',\n",
       "       'V2', 'V2', 'V2', 'V2', 'V2', 'V2', 'V2', 'V2', 'V2', 'V2', 'V2',\n",
       "       'V2', 'V2', 'V2', 'V3', 'V3', 'V3', 'V3', 'V3', 'V3', 'V3', 'V3',\n",
       "       'V3', 'V3', 'V3', 'V3', 'V3', 'V3', 'V3', 'V3', 'V3', 'V3', 'V3',\n",
       "       'V3', 'V3', 'V3', 'V3', 'V3', 'V3', 'V3', 'V3', 'V3', 'V3', 'V3',\n",
       "       'V3', 'V3', 'V3', 'V3', 'V3', 'V3', 'V3', 'V3', 'V3', 'V3', 'V4',\n",
       "       'V4', 'V4', 'V4', 'V4', 'V4', 'V4', 'V4', 'V4', 'V4', 'V4', 'V4',\n",
       "       'V4', 'V4', 'V4', 'V4', 'V4', 'V4', 'V4', 'V4', 'V4', 'V4', 'V4',\n",
       "       'V4', 'V4', 'V4', 'V4', 'V4', 'V4', 'V4', 'V4', 'V4', 'V4', 'V4',\n",
       "       'V4', 'V4', 'V4', 'V4', 'V4', 'V4', 'V5', 'V5', 'V5', 'V5', 'V5',\n",
       "       'V5', 'V5', 'V5', 'V5', 'V5', 'V5', 'V5', 'V5', 'V5', 'V5', 'V5',\n",
       "       'V5', 'V5', 'V5', 'V5', 'V5', 'V5', 'V5', 'V5', 'V5', 'V5', 'V5',\n",
       "       'V5', 'V5', 'V5', 'V5', 'V5', 'V5', 'V5', 'V5', 'V5', 'V5', 'V5',\n",
       "       'V5', 'V5', 'V6', 'V6', 'V6', 'V6', 'V6', 'V6', 'V6', 'V6', 'V6',\n",
       "       'V6', 'V6', 'V6', 'V6', 'V6', 'V6', 'V6', 'V6', 'V6', 'V6', 'V6',\n",
       "       'V6', 'V6', 'V6', 'V6', 'V6', 'V6', 'V6', 'V6', 'V6', 'V6', 'V6',\n",
       "       'V6', 'V6', 'V6', 'V6', 'V6', 'V6', 'V6', 'V6', 'V6', 'V7', 'V7',\n",
       "       'V7', 'V7', 'V7', 'V7', 'V7', 'V7', 'V7', 'V7', 'V7', 'V7', 'V7',\n",
       "       'V7', 'V7', 'V7', 'V7', 'V7', 'V7', 'V7', 'V7', 'V7', 'V7', 'V7',\n",
       "       'V7', 'V7', 'V7', 'V7', 'V7', 'V7', 'V7', 'V7', 'V7', 'V7', 'V7',\n",
       "       'V7', 'V7', 'V7', 'V7', 'V7', 'V8', 'V8', 'V8', 'V8', 'V8', 'V8',\n",
       "       'V8', 'V8', 'V8', 'V8', 'V8', 'V8', 'V8', 'V8', 'V8', 'V8', 'V8',\n",
       "       'V8', 'V8', 'V8', 'V8', 'V8', 'V8', 'V8', 'V8', 'V8', 'V8', 'V8',\n",
       "       'V8', 'V8', 'V8', 'V8', 'V8', 'V8', 'V8', 'V8', 'V8', 'V8', 'V8',\n",
       "       'V8', 'V9', 'V9', 'V9', 'V9', 'V9', 'V9', 'V9', 'V9', 'V9', 'V9',\n",
       "       'V9', 'V9', 'V9', 'V9', 'V9', 'V9', 'V9', 'V9', 'V9', 'V9', 'V9',\n",
       "       'V9', 'V9', 'V9', 'V9', 'V9', 'V9', 'V9', 'V9', 'V9', 'V9', 'V9',\n",
       "       'V9', 'V9', 'V9', 'V9', 'V9', 'V9', 'V9', 'V9', 'V10', 'V10',\n",
       "       'V10', 'V10', 'V10', 'V10', 'V10', 'V10', 'V10', 'V10', 'V10',\n",
       "       'V10', 'V10', 'V10', 'V10', 'V10', 'V10', 'V10', 'V10', 'V10',\n",
       "       'V10', 'V10', 'V10', 'V10', 'V10', 'V10', 'V10', 'V10', 'V10',\n",
       "       'V10', 'V10', 'V10', 'V10', 'V10', 'V10', 'V10', 'V10', 'V10',\n",
       "       'V10', 'V10'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting the class V9 as 0 and all other classes as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(labels)):\n",
    "    if(labels[i]==\"V9\"):\n",
    "        labels[i]=0\n",
    "    else:\n",
    "        labels[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalisation technique used-Min-Max Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 19)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler=MinMaxScaler((-1,1))\n",
    "x=scaler.fit_transform(features)\n",
    "y=labels\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the training dataset in the ratio 7:3 \n",
    "#### 70% is used for training the model and 30% is used for validation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x, y, test_size=0.3, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(19,)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.asarray(x_train).astype(np.int)\n",
    "\n",
    "y_train=np.asarray(y_train).astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>P3</th>\n",
       "      <th>P4</th>\n",
       "      <th>P5</th>\n",
       "      <th>P6</th>\n",
       "      <th>P7</th>\n",
       "      <th>P8</th>\n",
       "      <th>P9</th>\n",
       "      <th>P10</th>\n",
       "      <th>...</th>\n",
       "      <th>P12</th>\n",
       "      <th>P13</th>\n",
       "      <th>P14</th>\n",
       "      <th>P15</th>\n",
       "      <th>P16</th>\n",
       "      <th>P17</th>\n",
       "      <th>P18</th>\n",
       "      <th>P19</th>\n",
       "      <th>P20</th>\n",
       "      <th>Target Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.85</td>\n",
       "      <td>6.98</td>\n",
       "      <td>1.47</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.1</td>\n",
       "      <td>...</td>\n",
       "      <td>52.7</td>\n",
       "      <td>8.94</td>\n",
       "      <td>6.17</td>\n",
       "      <td>6.43</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.07</td>\n",
       "      <td>25.3</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.90</td>\n",
       "      <td>7.92</td>\n",
       "      <td>1.72</td>\n",
       "      <td>3.40</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.7</td>\n",
       "      <td>...</td>\n",
       "      <td>54.8</td>\n",
       "      <td>11.04</td>\n",
       "      <td>8.05</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.18</td>\n",
       "      <td>27.2</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.1</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.31</td>\n",
       "      <td>7.61</td>\n",
       "      <td>1.44</td>\n",
       "      <td>3.37</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>...</td>\n",
       "      <td>50.7</td>\n",
       "      <td>7.63</td>\n",
       "      <td>4.85</td>\n",
       "      <td>4.76</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>25.7</td>\n",
       "      <td>5.3</td>\n",
       "      <td>3.1</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.71</td>\n",
       "      <td>7.71</td>\n",
       "      <td>1.61</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.7</td>\n",
       "      <td>...</td>\n",
       "      <td>55.6</td>\n",
       "      <td>8.74</td>\n",
       "      <td>6.43</td>\n",
       "      <td>6.56</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.20</td>\n",
       "      <td>27.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.26</td>\n",
       "      <td>7.42</td>\n",
       "      <td>1.44</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.6</td>\n",
       "      <td>...</td>\n",
       "      <td>53.6</td>\n",
       "      <td>9.89</td>\n",
       "      <td>6.97</td>\n",
       "      <td>6.83</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.11</td>\n",
       "      <td>26.7</td>\n",
       "      <td>4.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>3.26</td>\n",
       "      <td>7.20</td>\n",
       "      <td>1.61</td>\n",
       "      <td>3.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.87</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>173.5</td>\n",
       "      <td>...</td>\n",
       "      <td>129.9</td>\n",
       "      <td>11.61</td>\n",
       "      <td>13.96</td>\n",
       "      <td>13.74</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>69.6</td>\n",
       "      <td>4.6</td>\n",
       "      <td>15.0</td>\n",
       "      <td>V10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>3.56</td>\n",
       "      <td>7.46</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.86</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172.7</td>\n",
       "      <td>...</td>\n",
       "      <td>140.3</td>\n",
       "      <td>10.51</td>\n",
       "      <td>12.70</td>\n",
       "      <td>12.59</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>70.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>11.0</td>\n",
       "      <td>V10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>4.73</td>\n",
       "      <td>8.73</td>\n",
       "      <td>1.95</td>\n",
       "      <td>3.59</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>171.6</td>\n",
       "      <td>...</td>\n",
       "      <td>147.7</td>\n",
       "      <td>14.37</td>\n",
       "      <td>15.69</td>\n",
       "      <td>14.97</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>71.5</td>\n",
       "      <td>2.9</td>\n",
       "      <td>7.9</td>\n",
       "      <td>V10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>3.66</td>\n",
       "      <td>7.63</td>\n",
       "      <td>1.70</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.9</td>\n",
       "      <td>...</td>\n",
       "      <td>131.2</td>\n",
       "      <td>11.23</td>\n",
       "      <td>16.31</td>\n",
       "      <td>18.52</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>70.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>15.9</td>\n",
       "      <td>V10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>3.78</td>\n",
       "      <td>7.44</td>\n",
       "      <td>1.69</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.83</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>...</td>\n",
       "      <td>124.5</td>\n",
       "      <td>14.65</td>\n",
       "      <td>17.17</td>\n",
       "      <td>17.30</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.03</td>\n",
       "      <td>69.1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>V10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      P1    P2    P3    P4    P5    P6    P7    P8   P9    P10  ...    P12  \\\n",
       "0   2.85  6.98  1.47  3.03  0.04  2.06  0.74  0.64  0.0   57.1  ...   52.7   \n",
       "1   3.90  7.92  1.72  3.40  0.05  1.97  0.78  0.66  0.0   63.7  ...   54.8   \n",
       "2   3.31  7.61  1.44  3.37  0.04  2.34  0.72  0.68  0.0   61.0  ...   50.7   \n",
       "3   3.71  7.71  1.61  3.27  0.05  2.03  0.79  0.71  0.0   61.7  ...   55.6   \n",
       "4   3.26  7.42  1.44  3.27  0.05  2.27  0.74  0.69  0.0   61.6  ...   53.6   \n",
       "..   ...   ...   ...   ...   ...   ...   ...   ...  ...    ...  ...    ...   \n",
       "95  3.26  7.20  1.61  3.01  0.05  1.87  0.79  0.67  0.0  173.5  ...  129.9   \n",
       "96  3.56  7.46  1.66  3.10  0.07  1.86  0.81  0.69  0.0  172.7  ...  140.3   \n",
       "97  4.73  8.73  1.95  3.59  0.03  1.84  0.78  0.67  0.0  171.6  ...  147.7   \n",
       "98  3.66  7.63  1.70  3.14  0.05  1.85  0.79  0.68  0.0  175.9  ...  131.2   \n",
       "99  3.78  7.44  1.69  3.10  0.05  1.83  0.86  0.72  0.0  174.0  ...  124.5   \n",
       "\n",
       "      P13    P14    P15   P16   P17   P18  P19   P20  Target Label  \n",
       "0    8.94   6.17   6.43  0.19  0.07  25.3  3.7   1.0            V1  \n",
       "1   11.04   8.05   7.70  0.71  0.18  27.2  5.5   2.1            V1  \n",
       "2    7.63   4.85   4.76  0.07  0.02  25.7  5.3   3.1            V1  \n",
       "3    8.74   6.43   6.56  0.76  0.20  27.4  3.7   1.8            V1  \n",
       "4    9.89   6.97   6.83  0.34  0.11  26.7  4.6   2.2            V1  \n",
       "..    ...    ...    ...   ...   ...   ...  ...   ...           ...  \n",
       "95  11.61  13.96  13.74  0.01  0.00  69.6  4.6  15.0           V10  \n",
       "96  10.51  12.70  12.59  0.03  0.01  70.8  3.4  11.0           V10  \n",
       "97  14.37  15.69  14.97  0.09  0.02  71.5  2.9   7.9           V10  \n",
       "98  11.23  16.31  18.52  0.03  0.01  70.7  3.8  15.9           V10  \n",
       "99  14.65  17.17  17.30  0.11  0.03  69.1  5.0  17.3           V10  \n",
       "\n",
       "[100 rows x 21 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_excel('Multiclass-dataset.xlsx',sheet_name='TestingSet')\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Preprocessing the testing model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=df.loc[:,df.columns!='Target Label'].values[:,1:]\n",
    "labels=df.loc[:,'Target Label'].values\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If label is V9 then it is converted into one and all the other labels are replaced with a 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(labels)):\n",
    "    if(labels[i]==\"V9\"):\n",
    "        labels[i]=0\n",
    "    else:\n",
    "        labels[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler((-1,1))\n",
    "x_test=scaler.fit_transform(features)\n",
    "y_test=labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=np.asarray(x_test).astype(np.float)\n",
    "\n",
    "y_test=np.asarray(y_test).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 280 samples, validate on 400 samples\n",
      "Epoch 1/45\n",
      "280/280 - 1s - loss: 0.6202 - accuracy: 0.7964 - val_loss: 0.4478 - val_accuracy: 1.0000\n",
      "Epoch 2/45\n",
      "280/280 - 0s - loss: 0.4629 - accuracy: 0.8964 - val_loss: 0.2365 - val_accuracy: 1.0000\n",
      "Epoch 3/45\n",
      "280/280 - 0s - loss: 0.3963 - accuracy: 0.8964 - val_loss: 0.1172 - val_accuracy: 1.0000\n",
      "Epoch 4/45\n",
      "280/280 - 0s - loss: 0.3615 - accuracy: 0.8964 - val_loss: 0.0749 - val_accuracy: 1.0000\n",
      "Epoch 5/45\n",
      "280/280 - 0s - loss: 0.3489 - accuracy: 0.8964 - val_loss: 0.0631 - val_accuracy: 1.0000\n",
      "Epoch 6/45\n",
      "280/280 - 0s - loss: 0.3375 - accuracy: 0.8964 - val_loss: 0.0576 - val_accuracy: 1.0000\n",
      "Epoch 7/45\n",
      "280/280 - 0s - loss: 0.3267 - accuracy: 0.8964 - val_loss: 0.0513 - val_accuracy: 1.0000\n",
      "Epoch 8/45\n",
      "280/280 - 0s - loss: 0.3201 - accuracy: 0.8964 - val_loss: 0.0492 - val_accuracy: 1.0000\n",
      "Epoch 9/45\n",
      "280/280 - 0s - loss: 0.3131 - accuracy: 0.8964 - val_loss: 0.0389 - val_accuracy: 1.0000\n",
      "Epoch 10/45\n",
      "280/280 - 0s - loss: 0.3095 - accuracy: 0.8964 - val_loss: 0.0317 - val_accuracy: 1.0000\n",
      "Epoch 11/45\n",
      "280/280 - 0s - loss: 0.3053 - accuracy: 0.8964 - val_loss: 0.0286 - val_accuracy: 1.0000\n",
      "Epoch 12/45\n",
      "280/280 - 0s - loss: 0.3031 - accuracy: 0.8964 - val_loss: 0.0229 - val_accuracy: 1.0000\n",
      "Epoch 13/45\n",
      "280/280 - 0s - loss: 0.3013 - accuracy: 0.8964 - val_loss: 0.0203 - val_accuracy: 1.0000\n",
      "Epoch 14/45\n",
      "280/280 - 0s - loss: 0.2993 - accuracy: 0.8964 - val_loss: 0.0158 - val_accuracy: 1.0000\n",
      "Epoch 15/45\n",
      "280/280 - 0s - loss: 0.2976 - accuracy: 0.8964 - val_loss: 0.0136 - val_accuracy: 1.0000\n",
      "Epoch 16/45\n",
      "280/280 - 0s - loss: 0.2961 - accuracy: 0.8964 - val_loss: 0.0118 - val_accuracy: 1.0000\n",
      "Epoch 17/45\n",
      "280/280 - 0s - loss: 0.2950 - accuracy: 0.8964 - val_loss: 0.0097 - val_accuracy: 1.0000\n",
      "Epoch 18/45\n",
      "280/280 - 0s - loss: 0.2943 - accuracy: 0.8964 - val_loss: 0.0093 - val_accuracy: 1.0000\n",
      "Epoch 19/45\n",
      "280/280 - 0s - loss: 0.2937 - accuracy: 0.8964 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
      "Epoch 20/45\n",
      "280/280 - 0s - loss: 0.2919 - accuracy: 0.8964 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
      "Epoch 21/45\n",
      "280/280 - 0s - loss: 0.2912 - accuracy: 0.8964 - val_loss: 0.0057 - val_accuracy: 1.0000\n",
      "Epoch 22/45\n",
      "280/280 - 0s - loss: 0.2947 - accuracy: 0.8964 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
      "Epoch 23/45\n",
      "280/280 - 0s - loss: 0.2938 - accuracy: 0.8964 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
      "Epoch 24/45\n",
      "280/280 - 0s - loss: 0.2910 - accuracy: 0.8964 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
      "Epoch 25/45\n",
      "280/280 - 0s - loss: 0.2902 - accuracy: 0.8964 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 26/45\n",
      "280/280 - 0s - loss: 0.2902 - accuracy: 0.8964 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 27/45\n",
      "280/280 - 0s - loss: 0.2889 - accuracy: 0.8964 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 28/45\n",
      "280/280 - 0s - loss: 0.2911 - accuracy: 0.8964 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 29/45\n",
      "280/280 - 0s - loss: 0.2888 - accuracy: 0.8964 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 30/45\n",
      "280/280 - 0s - loss: 0.2882 - accuracy: 0.8964 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 31/45\n",
      "280/280 - 0s - loss: 0.2878 - accuracy: 0.8964 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 32/45\n",
      "280/280 - 0s - loss: 0.2886 - accuracy: 0.8964 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 33/45\n",
      "280/280 - 0s - loss: 0.2875 - accuracy: 0.8964 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 34/45\n",
      "280/280 - 0s - loss: 0.2876 - accuracy: 0.8964 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 35/45\n",
      "280/280 - 0s - loss: 0.2876 - accuracy: 0.8964 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 36/45\n",
      "280/280 - 0s - loss: 0.2887 - accuracy: 0.8964 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 37/45\n",
      "280/280 - 0s - loss: 0.2885 - accuracy: 0.8964 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 38/45\n",
      "280/280 - 0s - loss: 0.2868 - accuracy: 0.8964 - val_loss: 9.5384e-04 - val_accuracy: 1.0000\n",
      "Epoch 39/45\n",
      "280/280 - 0s - loss: 0.2870 - accuracy: 0.8964 - val_loss: 8.4104e-04 - val_accuracy: 1.0000\n",
      "Epoch 40/45\n",
      "280/280 - 0s - loss: 0.2868 - accuracy: 0.8964 - val_loss: 8.3022e-04 - val_accuracy: 1.0000\n",
      "Epoch 41/45\n",
      "280/280 - 0s - loss: 0.2865 - accuracy: 0.8964 - val_loss: 7.8202e-04 - val_accuracy: 1.0000\n",
      "Epoch 42/45\n",
      "280/280 - 0s - loss: 0.2871 - accuracy: 0.8964 - val_loss: 7.6002e-04 - val_accuracy: 1.0000\n",
      "Epoch 43/45\n",
      "280/280 - 0s - loss: 0.2872 - accuracy: 0.8964 - val_loss: 6.4653e-04 - val_accuracy: 1.0000\n",
      "Epoch 44/45\n",
      "280/280 - 0s - loss: 0.2895 - accuracy: 0.8964 - val_loss: 5.3942e-04 - val_accuracy: 1.0000\n",
      "Epoch 45/45\n",
      "280/280 - 0s - loss: 0.2860 - accuracy: 0.8964 - val_loss: 6.0646e-04 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,batch_size=32,epochs=45,verbose=2,validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0006064609612803906\n",
      "Test accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference: Accuracy=100%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
