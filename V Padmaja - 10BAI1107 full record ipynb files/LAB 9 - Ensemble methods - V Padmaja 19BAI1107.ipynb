{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oNvM2PO0l0U"
   },
   "source": [
    "# Ensemble Methods - Padmaja 19BAI1107"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2jiLlfQY02fO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report,confusion_matrix,homogeneity_score,completeness_score,v_measure_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7hBq2es3k0B"
   },
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "L_jUQLMx0_jg"
   },
   "outputs": [],
   "source": [
    "dftrain=pd.read_excel('Multiclass-dataset.xlsx', sheet_name='trainingset',engine='openpyxl')\n",
    "dftest=pd.read_excel('Multiclass-dataset.xlsx', sheet_name='TestingSet', engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "EL2UojHB1ZQg",
    "outputId": "f4580557-7de9-4ac3-c3f1-9c56c3a4fe7c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>P3</th>\n",
       "      <th>P4</th>\n",
       "      <th>P5</th>\n",
       "      <th>P6</th>\n",
       "      <th>P7</th>\n",
       "      <th>P8</th>\n",
       "      <th>P9</th>\n",
       "      <th>P10</th>\n",
       "      <th>...</th>\n",
       "      <th>P12</th>\n",
       "      <th>P13</th>\n",
       "      <th>P14</th>\n",
       "      <th>P15</th>\n",
       "      <th>P16</th>\n",
       "      <th>P17</th>\n",
       "      <th>P18</th>\n",
       "      <th>P19</th>\n",
       "      <th>P20</th>\n",
       "      <th>Target Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.30</td>\n",
       "      <td>7.44</td>\n",
       "      <td>1.52</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.07</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.8</td>\n",
       "      <td>...</td>\n",
       "      <td>50.7</td>\n",
       "      <td>6.55</td>\n",
       "      <td>4.09</td>\n",
       "      <td>4.26</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24.7</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.43</td>\n",
       "      <td>7.63</td>\n",
       "      <td>1.63</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.8</td>\n",
       "      <td>...</td>\n",
       "      <td>47.9</td>\n",
       "      <td>8.35</td>\n",
       "      <td>5.08</td>\n",
       "      <td>5.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23.3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.41</td>\n",
       "      <td>7.32</td>\n",
       "      <td>1.52</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.07</td>\n",
       "      <td>2.09</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>...</td>\n",
       "      <td>54.4</td>\n",
       "      <td>9.27</td>\n",
       "      <td>6.85</td>\n",
       "      <td>7.14</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.06</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.78</td>\n",
       "      <td>7.85</td>\n",
       "      <td>1.69</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.7</td>\n",
       "      <td>...</td>\n",
       "      <td>48.9</td>\n",
       "      <td>10.26</td>\n",
       "      <td>5.96</td>\n",
       "      <td>5.47</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>24.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.90</td>\n",
       "      <td>7.99</td>\n",
       "      <td>1.61</td>\n",
       "      <td>3.43</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.1</td>\n",
       "      <td>...</td>\n",
       "      <td>54.1</td>\n",
       "      <td>8.19</td>\n",
       "      <td>5.81</td>\n",
       "      <td>4.72</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.16</td>\n",
       "      <td>26.8</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.1</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     P1    P2    P3    P4    P5    P6    P7    P8   P9   P10  ...   P12  \\\n",
       "0  3.30  7.44  1.52  3.27  0.07  2.14  0.75  0.66  0.0  54.8  ...  50.7   \n",
       "1  3.43  7.63  1.63  3.27  0.05  2.01  0.74  0.65  0.0  51.8  ...  47.9   \n",
       "2  3.41  7.32  1.52  3.18  0.07  2.09  0.80  0.70  0.0  54.0  ...  54.4   \n",
       "3  3.78  7.85  1.69  3.35  0.03  1.98  0.77  0.67  0.0  57.7  ...  48.9   \n",
       "4  3.90  7.99  1.61  3.43  0.02  2.14  0.77  0.71  0.0  59.1  ...  54.1   \n",
       "\n",
       "     P13   P14   P15   P16   P17   P18  P19  P20  Target Label  \n",
       "0   6.55  4.09  4.26  0.01  0.00  24.7  2.7  1.6            V1  \n",
       "1   8.35  5.08  5.01  0.01  0.00  23.3  2.3  1.8            V1  \n",
       "2   9.27  6.85  7.14  0.19  0.06  25.0  2.5 -0.9            V1  \n",
       "3  10.26  5.96  5.47  0.05  0.01  24.1  5.6  2.1            V1  \n",
       "4   8.19  5.81  4.72  0.64  0.16  26.8  2.5  2.1            V1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>P3</th>\n",
       "      <th>P4</th>\n",
       "      <th>P5</th>\n",
       "      <th>P6</th>\n",
       "      <th>P7</th>\n",
       "      <th>P8</th>\n",
       "      <th>P9</th>\n",
       "      <th>P10</th>\n",
       "      <th>...</th>\n",
       "      <th>P12</th>\n",
       "      <th>P13</th>\n",
       "      <th>P14</th>\n",
       "      <th>P15</th>\n",
       "      <th>P16</th>\n",
       "      <th>P17</th>\n",
       "      <th>P18</th>\n",
       "      <th>P19</th>\n",
       "      <th>P20</th>\n",
       "      <th>Target Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.85</td>\n",
       "      <td>6.98</td>\n",
       "      <td>1.47</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.1</td>\n",
       "      <td>...</td>\n",
       "      <td>52.7</td>\n",
       "      <td>8.94</td>\n",
       "      <td>6.17</td>\n",
       "      <td>6.43</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.07</td>\n",
       "      <td>25.3</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.90</td>\n",
       "      <td>7.92</td>\n",
       "      <td>1.72</td>\n",
       "      <td>3.40</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.7</td>\n",
       "      <td>...</td>\n",
       "      <td>54.8</td>\n",
       "      <td>11.04</td>\n",
       "      <td>8.05</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.18</td>\n",
       "      <td>27.2</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.1</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.31</td>\n",
       "      <td>7.61</td>\n",
       "      <td>1.44</td>\n",
       "      <td>3.37</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>...</td>\n",
       "      <td>50.7</td>\n",
       "      <td>7.63</td>\n",
       "      <td>4.85</td>\n",
       "      <td>4.76</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>25.7</td>\n",
       "      <td>5.3</td>\n",
       "      <td>3.1</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.71</td>\n",
       "      <td>7.71</td>\n",
       "      <td>1.61</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.7</td>\n",
       "      <td>...</td>\n",
       "      <td>55.6</td>\n",
       "      <td>8.74</td>\n",
       "      <td>6.43</td>\n",
       "      <td>6.56</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.20</td>\n",
       "      <td>27.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.26</td>\n",
       "      <td>7.42</td>\n",
       "      <td>1.44</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.6</td>\n",
       "      <td>...</td>\n",
       "      <td>53.6</td>\n",
       "      <td>9.89</td>\n",
       "      <td>6.97</td>\n",
       "      <td>6.83</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.11</td>\n",
       "      <td>26.7</td>\n",
       "      <td>4.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     P1    P2    P3    P4    P5    P6    P7    P8   P9   P10  ...   P12  \\\n",
       "0  2.85  6.98  1.47  3.03  0.04  2.06  0.74  0.64  0.0  57.1  ...  52.7   \n",
       "1  3.90  7.92  1.72  3.40  0.05  1.97  0.78  0.66  0.0  63.7  ...  54.8   \n",
       "2  3.31  7.61  1.44  3.37  0.04  2.34  0.72  0.68  0.0  61.0  ...  50.7   \n",
       "3  3.71  7.71  1.61  3.27  0.05  2.03  0.79  0.71  0.0  61.7  ...  55.6   \n",
       "4  3.26  7.42  1.44  3.27  0.05  2.27  0.74  0.69  0.0  61.6  ...  53.6   \n",
       "\n",
       "     P13   P14   P15   P16   P17   P18  P19  P20  Target Label  \n",
       "0   8.94  6.17  6.43  0.19  0.07  25.3  3.7  1.0            V1  \n",
       "1  11.04  8.05  7.70  0.71  0.18  27.2  5.5  2.1            V1  \n",
       "2   7.63  4.85  4.76  0.07  0.02  25.7  5.3  3.1            V1  \n",
       "3   8.74  6.43  6.56  0.76  0.20  27.4  3.7  1.8            V1  \n",
       "4   9.89  6.97  6.83  0.34  0.11  26.7  4.6  2.2            V1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "5H7Qv-Bt1cOd"
   },
   "outputs": [],
   "source": [
    "dict_diff={\n",
    "    'V1':1,\n",
    "    'V2':2,\n",
    "    'V3':3,\n",
    "    'V4':4,\n",
    "    'V5':5,\n",
    "    'V6':6,\n",
    "    'V7':7,\n",
    "    'V8':8,\n",
    "    'V9':9,\n",
    "    'V10':10\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1EKg9rQ3qoq"
   },
   "source": [
    "### Changing the Target Label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ytw9LqqN1elW"
   },
   "outputs": [],
   "source": [
    "dftest['Target Label']=dftest['Target Label'].apply(lambda x:dict_diff[x])\n",
    "dftrain['Target Label']=dftrain['Target Label'].apply(lambda x:dict_diff[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LnonO8Pu37w4",
    "outputId": "0b327ad4-3a32-4e60-b820-cb30839701c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,\n",
       "        3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "        3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "        3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "        4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "        4,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
       "        5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
       "        5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "        7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "        7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
       "        9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
       "        9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
       "        9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 10, 10, 10, 10, 10, 10, 10, 10], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=dftrain.loc[:,dftrain.columns!='Target Label'].values[:,1:]\n",
    "labels=dftrain.loc[:,'Target Label'].values\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  4,  4,  4,\n",
       "        4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "        7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,\n",
       "        9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features1=dftest.loc[:,dftest.columns!='Target Label'].values[:,1:]\n",
    "labels1=dftest.loc[:,'Target Label'].values\n",
    "labels1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yzBYClo4LV4"
   },
   "source": [
    "### Data Preprocessing using MinMax Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCSdyxbU4SjK",
    "outputId": "8219b850-9473-42a1-cc07-08ba24fba615"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 19)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler=MinMaxScaler((-1,1))\n",
    "x_train=scaler.fit_transform(features)\n",
    "y_train=labels\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 19)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler=MinMaxScaler((-1,1))\n",
    "x_test=scaler.fit_transform(features1)\n",
    "y_test=labels1\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOOSTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXIHD-Y94g7d"
   },
   "source": [
    "## 1. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqaIB5nW4lzn"
   },
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4qMb7E724oWS",
    "outputId": "0a2114a9-e710-4f03-e352-510c7f81b4ba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:49:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=4, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = XGBClassifier()\n",
    "model1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ORCm-9Hg4rYs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\data.py:115: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"memory consumption\")\n"
     ]
    }
   ],
   "source": [
    "y_pred1 = model1.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qV5wWigH4vjs",
    "outputId": "46de1dbc-87b3-4386-fa88-65e6a957957d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JAUBrEU45h9"
   },
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "dq6wMfY546th"
   },
   "outputs": [],
   "source": [
    "param_grid = {'booster':['gbtree','gblinear','dart'],'max_depth':[1,2,3,4,5,6],'sampling_method':['uniform','gradient_based']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "fgQQ51XX4_IY"
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(XGBClassifier(),param_grid,refit=True,verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uYlyDRHR5BV4",
    "outputId": "7d852e2b-b4b3-4a9a-f79e-9c486100652f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "[20:50:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END booster=gbtree, max_depth=1, sampling_method=uniform; total time=   0.5s\n",
      "[20:50:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=gbtree, max_depth=1, sampling_method=uniform; total time=   0.5s\n",
      "[20:50:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END booster=gbtree, max_depth=1, sampling_method=uniform; total time=   0.5s\n",
      "[20:50:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END booster=gbtree, max_depth=1, sampling_method=uniform; total time=   0.5s\n",
      "[20:50:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END booster=gbtree, max_depth=1, sampling_method=uniform; total time=   0.4s\n",
      "[20:50:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END booster=gbtree, max_depth=1, sampling_method=gradient_based; total time=   0.4s\n",
      "[20:50:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=gbtree, max_depth=1, sampling_method=gradient_based; total time=   0.5s\n",
      "[20:50:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END booster=gbtree, max_depth=1, sampling_method=gradient_based; total time=   0.5s\n",
      "[20:50:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END booster=gbtree, max_depth=1, sampling_method=gradient_based; total time=   0.5s\n",
      "[20:50:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END booster=gbtree, max_depth=1, sampling_method=gradient_based; total time=   0.5s\n",
      "[20:50:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END booster=gbtree, max_depth=2, sampling_method=uniform; total time=   0.6s\n",
      "[20:50:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=gbtree, max_depth=2, sampling_method=uniform; total time=   0.7s\n",
      "[20:50:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END booster=gbtree, max_depth=2, sampling_method=uniform; total time=   0.7s\n",
      "[20:50:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END booster=gbtree, max_depth=2, sampling_method=uniform; total time=   0.7s\n",
      "[20:50:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END booster=gbtree, max_depth=2, sampling_method=uniform; total time=   0.7s\n",
      "[20:50:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END booster=gbtree, max_depth=2, sampling_method=gradient_based; total time=   0.6s\n",
      "[20:50:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=gbtree, max_depth=2, sampling_method=gradient_based; total time=   0.7s\n",
      "[20:50:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END booster=gbtree, max_depth=2, sampling_method=gradient_based; total time=   0.7s\n",
      "[20:50:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END booster=gbtree, max_depth=2, sampling_method=gradient_based; total time=   0.7s\n",
      "[20:50:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END booster=gbtree, max_depth=2, sampling_method=gradient_based; total time=   0.7s\n",
      "[20:50:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END booster=gbtree, max_depth=3, sampling_method=uniform; total time=   0.7s\n",
      "[20:50:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=gbtree, max_depth=3, sampling_method=uniform; total time=   0.8s\n",
      "[20:50:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END booster=gbtree, max_depth=3, sampling_method=uniform; total time=   0.8s\n",
      "[20:50:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END booster=gbtree, max_depth=3, sampling_method=uniform; total time=   0.8s\n",
      "[20:50:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END booster=gbtree, max_depth=3, sampling_method=uniform; total time=   0.7s\n",
      "[20:50:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END booster=gbtree, max_depth=3, sampling_method=gradient_based; total time=   0.7s\n",
      "[20:50:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=gbtree, max_depth=3, sampling_method=gradient_based; total time=   0.8s\n",
      "[20:50:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END booster=gbtree, max_depth=3, sampling_method=gradient_based; total time=   0.8s\n",
      "[20:50:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END booster=gbtree, max_depth=3, sampling_method=gradient_based; total time=   0.8s\n",
      "[20:50:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END booster=gbtree, max_depth=3, sampling_method=gradient_based; total time=   0.8s\n",
      "[20:50:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END booster=gbtree, max_depth=4, sampling_method=uniform; total time=   0.7s\n",
      "[20:50:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=gbtree, max_depth=4, sampling_method=uniform; total time=   0.9s\n",
      "[20:50:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END booster=gbtree, max_depth=4, sampling_method=uniform; total time=   0.8s\n",
      "[20:50:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END booster=gbtree, max_depth=4, sampling_method=uniform; total time=   0.8s\n",
      "[20:50:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END booster=gbtree, max_depth=4, sampling_method=uniform; total time=   0.8s\n",
      "[20:50:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END booster=gbtree, max_depth=4, sampling_method=gradient_based; total time=   0.8s\n",
      "[20:50:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=gbtree, max_depth=4, sampling_method=gradient_based; total time=   0.9s\n",
      "[20:50:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END booster=gbtree, max_depth=4, sampling_method=gradient_based; total time=   0.9s\n",
      "[20:50:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END booster=gbtree, max_depth=4, sampling_method=gradient_based; total time=   0.8s\n",
      "[20:50:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END booster=gbtree, max_depth=4, sampling_method=gradient_based; total time=   0.8s\n",
      "[20:50:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END booster=gbtree, max_depth=5, sampling_method=uniform; total time=   0.8s\n",
      "[20:50:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=gbtree, max_depth=5, sampling_method=uniform; total time=   0.8s\n",
      "[20:50:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END booster=gbtree, max_depth=5, sampling_method=uniform; total time=   0.9s\n",
      "[20:50:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END booster=gbtree, max_depth=5, sampling_method=uniform; total time=   0.8s\n",
      "[20:50:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END booster=gbtree, max_depth=5, sampling_method=uniform; total time=   0.9s\n",
      "[20:50:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END booster=gbtree, max_depth=5, sampling_method=gradient_based; total time=   0.8s\n",
      "[20:50:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=gbtree, max_depth=5, sampling_method=gradient_based; total time=   0.9s\n",
      "[20:50:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END booster=gbtree, max_depth=5, sampling_method=gradient_based; total time=   0.8s\n",
      "[20:50:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END booster=gbtree, max_depth=5, sampling_method=gradient_based; total time=   0.8s\n",
      "[20:50:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END booster=gbtree, max_depth=5, sampling_method=gradient_based; total time=   0.9s\n",
      "[20:50:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END booster=gbtree, max_depth=6, sampling_method=uniform; total time=   0.8s\n",
      "[20:50:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=gbtree, max_depth=6, sampling_method=uniform; total time=   0.8s\n",
      "[20:50:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END booster=gbtree, max_depth=6, sampling_method=uniform; total time=   0.9s\n",
      "[20:50:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END booster=gbtree, max_depth=6, sampling_method=uniform; total time=   0.8s\n",
      "[20:50:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END booster=gbtree, max_depth=6, sampling_method=uniform; total time=   0.8s\n",
      "[20:50:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END booster=gbtree, max_depth=6, sampling_method=gradient_based; total time=   0.8s\n",
      "[20:50:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=gbtree, max_depth=6, sampling_method=gradient_based; total time=   0.8s\n",
      "[20:50:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END booster=gbtree, max_depth=6, sampling_method=gradient_based; total time=   0.8s\n",
      "[20:50:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END booster=gbtree, max_depth=6, sampling_method=gradient_based; total time=   0.8s\n",
      "[20:51:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END booster=gbtree, max_depth=6, sampling_method=gradient_based; total time=   0.9s\n",
      "[20:51:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5] END booster=gblinear, max_depth=1, sampling_method=uniform; total time=   0.1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[20:51:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=gblinear, max_depth=1, sampling_method=uniform; total time=   0.1s\n",
      "[20:51:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END booster=gblinear, max_depth=1, sampling_method=uniform; total time=   0.1s\n",
      "[20:51:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5] END booster=gblinear, max_depth=1, sampling_method=uniform; total time=   0.1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[20:51:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5] END booster=gblinear, max_depth=1, sampling_method=uniform; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5] END booster=gblinear, max_depth=1, sampling_method=gradient_based; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5] END booster=gblinear, max_depth=1, sampling_method=gradient_based; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5] END booster=gblinear, max_depth=1, sampling_method=gradient_based; total time=   0.1s\n",
      "[20:51:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END booster=gblinear, max_depth=1, sampling_method=gradient_based; total time=   0.1s\n",
      "[20:51:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5] END booster=gblinear, max_depth=1, sampling_method=gradient_based; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5] END booster=gblinear, max_depth=2, sampling_method=uniform; total time=   0.1s\n",
      "[20:51:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=gblinear, max_depth=2, sampling_method=uniform; total time=   0.1s\n",
      "[20:51:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5] END booster=gblinear, max_depth=2, sampling_method=uniform; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5] END booster=gblinear, max_depth=2, sampling_method=uniform; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5] END booster=gblinear, max_depth=2, sampling_method=uniform; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5] END booster=gblinear, max_depth=2, sampling_method=gradient_based; total time=   0.1s\n",
      "[20:51:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5] END booster=gblinear, max_depth=2, sampling_method=gradient_based; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5] END booster=gblinear, max_depth=2, sampling_method=gradient_based; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5] END booster=gblinear, max_depth=2, sampling_method=gradient_based; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5] END booster=gblinear, max_depth=2, sampling_method=gradient_based; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5] END booster=gblinear, max_depth=3, sampling_method=uniform; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5] END booster=gblinear, max_depth=3, sampling_method=uniform; total time=   0.1s\n",
      "[20:51:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5] END booster=gblinear, max_depth=3, sampling_method=uniform; total time=   0.1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[20:51:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END booster=gblinear, max_depth=3, sampling_method=uniform; total time=   0.1s\n",
      "[20:51:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5] END booster=gblinear, max_depth=3, sampling_method=uniform; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END booster=gblinear, max_depth=3, sampling_method=gradient_based; total time=   0.1s\n",
      "[20:51:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=gblinear, max_depth=3, sampling_method=gradient_based; total time=   0.1s\n",
      "[20:51:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END booster=gblinear, max_depth=3, sampling_method=gradient_based; total time=   0.1s\n",
      "[20:51:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5] END booster=gblinear, max_depth=3, sampling_method=gradient_based; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5] END booster=gblinear, max_depth=3, sampling_method=gradient_based; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5] END booster=gblinear, max_depth=4, sampling_method=uniform; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5] END booster=gblinear, max_depth=4, sampling_method=uniform; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5] END booster=gblinear, max_depth=4, sampling_method=uniform; total time=   0.1s\n",
      "[20:51:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5] END booster=gblinear, max_depth=4, sampling_method=uniform; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5] END booster=gblinear, max_depth=4, sampling_method=uniform; total time=   0.1s\n",
      "[20:51:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END booster=gblinear, max_depth=4, sampling_method=gradient_based; total time=   0.1s\n",
      "[20:51:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=gblinear, max_depth=4, sampling_method=gradient_based; total time=   0.1s\n",
      "[20:51:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END booster=gblinear, max_depth=4, sampling_method=gradient_based; total time=   0.1s\n",
      "[20:51:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END booster=gblinear, max_depth=4, sampling_method=gradient_based; total time=   0.1s\n",
      "[20:51:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5] END booster=gblinear, max_depth=4, sampling_method=gradient_based; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5] END booster=gblinear, max_depth=5, sampling_method=uniform; total time=   0.1s\n",
      "[20:51:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=gblinear, max_depth=5, sampling_method=uniform; total time=   0.1s\n",
      "[20:51:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5] END booster=gblinear, max_depth=5, sampling_method=uniform; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5] END booster=gblinear, max_depth=5, sampling_method=uniform; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5] END booster=gblinear, max_depth=5, sampling_method=uniform; total time=   0.1s\n",
      "[20:51:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5] END booster=gblinear, max_depth=5, sampling_method=gradient_based; total time=   0.1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[20:51:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=gblinear, max_depth=5, sampling_method=gradient_based; total time=   0.1s\n",
      "[20:51:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END booster=gblinear, max_depth=5, sampling_method=gradient_based; total time=   0.1s\n",
      "[20:51:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5] END booster=gblinear, max_depth=5, sampling_method=gradient_based; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5] END booster=gblinear, max_depth=5, sampling_method=gradient_based; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5] END booster=gblinear, max_depth=6, sampling_method=uniform; total time=   0.1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[20:51:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5] END booster=gblinear, max_depth=6, sampling_method=uniform; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5] END booster=gblinear, max_depth=6, sampling_method=uniform; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5] END booster=gblinear, max_depth=6, sampling_method=uniform; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5] END booster=gblinear, max_depth=6, sampling_method=uniform; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5] END booster=gblinear, max_depth=6, sampling_method=gradient_based; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5] END booster=gblinear, max_depth=6, sampling_method=gradient_based; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5] END booster=gblinear, max_depth=6, sampling_method=gradient_based; total time=   0.1s\n",
      "[20:51:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END booster=gblinear, max_depth=6, sampling_method=gradient_based; total time=   0.1s\n",
      "[20:51:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"max_depth\", \"sampling_method\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:51:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5] END booster=gblinear, max_depth=6, sampling_method=gradient_based; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5] END booster=dart, max_depth=1, sampling_method=uniform; total time=  10.7s\n",
      "[20:51:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=dart, max_depth=1, sampling_method=uniform; total time=  11.3s\n",
      "[20:51:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END booster=dart, max_depth=1, sampling_method=uniform; total time=  10.8s\n",
      "[20:51:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END booster=dart, max_depth=1, sampling_method=uniform; total time=  11.0s\n",
      "[20:51:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END booster=dart, max_depth=1, sampling_method=uniform; total time=  11.1s\n",
      "[20:52:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END booster=dart, max_depth=1, sampling_method=gradient_based; total time=   9.8s\n",
      "[20:52:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=dart, max_depth=1, sampling_method=gradient_based; total time=  10.2s\n",
      "[20:52:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END booster=dart, max_depth=1, sampling_method=gradient_based; total time=   9.0s\n",
      "[20:52:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END booster=dart, max_depth=1, sampling_method=gradient_based; total time=  10.2s\n",
      "[20:52:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END booster=dart, max_depth=1, sampling_method=gradient_based; total time=  11.1s\n",
      "[20:53:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END booster=dart, max_depth=2, sampling_method=uniform; total time=  10.2s\n",
      "[20:53:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=dart, max_depth=2, sampling_method=uniform; total time=  10.7s\n",
      "[20:53:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END booster=dart, max_depth=2, sampling_method=uniform; total time=  10.7s\n",
      "[20:53:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END booster=dart, max_depth=2, sampling_method=uniform; total time=  10.8s\n",
      "[20:53:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END booster=dart, max_depth=2, sampling_method=uniform; total time=  10.9s\n",
      "[20:53:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END booster=dart, max_depth=2, sampling_method=gradient_based; total time=   9.9s\n",
      "[20:54:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=dart, max_depth=2, sampling_method=gradient_based; total time=  10.4s\n",
      "[20:54:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END booster=dart, max_depth=2, sampling_method=gradient_based; total time=  11.3s\n",
      "[20:54:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END booster=dart, max_depth=2, sampling_method=gradient_based; total time=  12.4s\n",
      "[20:54:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END booster=dart, max_depth=2, sampling_method=gradient_based; total time=   9.7s\n",
      "[20:54:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END booster=dart, max_depth=3, sampling_method=uniform; total time=  12.0s\n",
      "[20:55:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=dart, max_depth=3, sampling_method=uniform; total time=  11.1s\n",
      "[20:55:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END booster=dart, max_depth=3, sampling_method=uniform; total time=  11.4s\n",
      "[20:55:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END booster=dart, max_depth=3, sampling_method=uniform; total time=  10.9s\n",
      "[20:55:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END booster=dart, max_depth=3, sampling_method=uniform; total time=  11.0s\n",
      "[20:55:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END booster=dart, max_depth=3, sampling_method=gradient_based; total time=  11.6s\n",
      "[20:55:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=dart, max_depth=3, sampling_method=gradient_based; total time=  11.3s\n",
      "[20:56:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END booster=dart, max_depth=3, sampling_method=gradient_based; total time=  11.0s\n",
      "[20:56:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END booster=dart, max_depth=3, sampling_method=gradient_based; total time=  11.5s\n",
      "[20:56:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END booster=dart, max_depth=3, sampling_method=gradient_based; total time=  10.9s\n",
      "[20:56:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END booster=dart, max_depth=4, sampling_method=uniform; total time=  11.1s\n",
      "[20:56:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=dart, max_depth=4, sampling_method=uniform; total time=  11.0s\n",
      "[20:57:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END booster=dart, max_depth=4, sampling_method=uniform; total time=  11.7s\n",
      "[20:57:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END booster=dart, max_depth=4, sampling_method=uniform; total time=  10.7s\n",
      "[20:57:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END booster=dart, max_depth=4, sampling_method=uniform; total time=  11.7s\n",
      "[20:57:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END booster=dart, max_depth=4, sampling_method=gradient_based; total time=  11.5s\n",
      "[20:57:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=dart, max_depth=4, sampling_method=gradient_based; total time=  11.2s\n",
      "[20:58:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END booster=dart, max_depth=4, sampling_method=gradient_based; total time=  11.4s\n",
      "[20:58:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END booster=dart, max_depth=4, sampling_method=gradient_based; total time=  10.9s\n",
      "[20:58:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END booster=dart, max_depth=4, sampling_method=gradient_based; total time=  11.1s\n",
      "[20:58:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END booster=dart, max_depth=5, sampling_method=uniform; total time=  11.0s\n",
      "[20:58:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=dart, max_depth=5, sampling_method=uniform; total time=  11.9s\n",
      "[20:58:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END booster=dart, max_depth=5, sampling_method=uniform; total time=  11.0s\n",
      "[20:59:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END booster=dart, max_depth=5, sampling_method=uniform; total time=  11.6s\n",
      "[20:59:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END booster=dart, max_depth=5, sampling_method=uniform; total time=  11.1s\n",
      "[20:59:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END booster=dart, max_depth=5, sampling_method=gradient_based; total time=  11.5s\n",
      "[20:59:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=dart, max_depth=5, sampling_method=gradient_based; total time=  11.2s\n",
      "[20:59:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END booster=dart, max_depth=5, sampling_method=gradient_based; total time=  11.6s\n",
      "[21:00:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END booster=dart, max_depth=5, sampling_method=gradient_based; total time=  11.1s\n",
      "[21:00:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END booster=dart, max_depth=5, sampling_method=gradient_based; total time=  11.7s\n",
      "[21:00:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END booster=dart, max_depth=6, sampling_method=uniform; total time=  11.6s\n",
      "[21:00:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=dart, max_depth=6, sampling_method=uniform; total time=  11.3s\n",
      "[21:00:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END booster=dart, max_depth=6, sampling_method=uniform; total time=  11.1s\n",
      "[21:01:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END booster=dart, max_depth=6, sampling_method=uniform; total time=  11.1s\n",
      "[21:01:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END booster=dart, max_depth=6, sampling_method=uniform; total time=  11.0s\n",
      "[21:01:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END booster=dart, max_depth=6, sampling_method=gradient_based; total time=  11.1s\n",
      "[21:01:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END booster=dart, max_depth=6, sampling_method=gradient_based; total time=  11.1s\n",
      "[21:01:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END booster=dart, max_depth=6, sampling_method=gradient_based; total time=  11.0s\n",
      "[21:01:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END booster=dart, max_depth=6, sampling_method=gradient_based; total time=  11.1s\n",
      "[21:02:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END booster=dart, max_depth=6, sampling_method=gradient_based; total time=  11.7s\n",
      "[21:02:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, random_state=None,\n",
       "                                     reg_alpha=None, reg_lambda=None,\n",
       "                                     scale_pos_weight=None, subsample=None,\n",
       "                                     tree_method=None, validate_parameters=None,\n",
       "                                     verbosity=None),\n",
       "             param_grid={'booster': ['gbtree', 'gblinear', 'dart'],\n",
       "                         'max_depth': [1, 2, 3, 4, 5, 6],\n",
       "                         'sampling_method': ['uniform', 'gradient_based']},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FHg0rdR35EPq",
    "outputId": "73dbf6e6-f9d5-480c-a34a-aac47611686a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'booster': 'gbtree', 'max_depth': 6, 'sampling_method': 'uniform'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "4kaBw_s65HLU"
   },
   "outputs": [],
   "source": [
    "model2=grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "0-iTJ1yr5LG9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\xgboost\\data.py:115: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"memory consumption\")\n"
     ]
    }
   ],
   "source": [
    "y_pred2= model2.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HyyoFxd15M75",
    "outputId": "1e48a101-0b27-4210-d490-80b07f33ba7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3cdsMXh5Xh-"
   },
   "source": [
    "## 2. Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmN91O3_5ciU"
   },
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "K9Jwbiga5dwr"
   },
   "outputs": [],
   "source": [
    "abc = AdaBoostClassifier(n_estimators=50,learning_rate=1)\n",
    "model3 = abc.fit(x_train, y_train)\n",
    "y_pred3 = model3.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XBt2rFOh5hD_",
    "outputId": "65221dc2-8574-4bae-fb85-4d3cfcedd4f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7rs-k4d5n87"
   },
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "UMeWz6ey5pGM"
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators':[10,20,30,40,50],'learning_rate':[1,2,3,4,5],'algorithm':['SAMME','SAMME.R']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "O7QS7rDk5tA7"
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(AdaBoostClassifier(),param_grid,refit=True,verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BzkFqtPO5yd1",
    "outputId": "cc55d176-dcfb-4e58-cdc6-63025a7ed605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=1, n_estimators=10; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=1, n_estimators=10; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=1, n_estimators=10; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=1, n_estimators=10; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=1, n_estimators=10; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=1, n_estimators=20; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=1, n_estimators=20; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=1, n_estimators=20; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=1, n_estimators=20; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=1, n_estimators=20; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=1, n_estimators=30; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=1, n_estimators=30; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=1, n_estimators=30; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=1, n_estimators=30; total time=   0.1s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=1, n_estimators=30; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=1, n_estimators=40; total time=   0.1s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=1, n_estimators=40; total time=   0.1s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=1, n_estimators=40; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=1, n_estimators=40; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=1, n_estimators=40; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=1, n_estimators=50; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=1, n_estimators=50; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=1, n_estimators=50; total time=   0.1s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=1, n_estimators=50; total time=   0.1s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=1, n_estimators=50; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=2, n_estimators=10; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=2, n_estimators=10; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=2, n_estimators=10; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=2, n_estimators=10; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=2, n_estimators=10; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=2, n_estimators=20; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=2, n_estimators=20; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=2, n_estimators=20; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=2, n_estimators=20; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=2, n_estimators=20; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=2, n_estimators=30; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=2, n_estimators=30; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=2, n_estimators=30; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=2, n_estimators=30; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=2, n_estimators=30; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=2, n_estimators=40; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=2, n_estimators=40; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=2, n_estimators=40; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=2, n_estimators=40; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=2, n_estimators=40; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=2, n_estimators=50; total time=   0.1s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=2, n_estimators=50; total time=   0.1s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=2, n_estimators=50; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=2, n_estimators=50; total time=   0.1s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=2, n_estimators=50; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=3, n_estimators=10; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=3, n_estimators=10; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=3, n_estimators=10; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=3, n_estimators=10; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=3, n_estimators=10; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=3, n_estimators=20; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=3, n_estimators=20; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=3, n_estimators=20; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=3, n_estimators=20; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=3, n_estimators=20; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=3, n_estimators=30; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=3, n_estimators=30; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=3, n_estimators=30; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=3, n_estimators=30; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=3, n_estimators=30; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=3, n_estimators=40; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=3, n_estimators=40; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=3, n_estimators=40; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=3, n_estimators=40; total time=   0.1s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=3, n_estimators=40; total time=   0.1s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=3, n_estimators=50; total time=   0.1s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=3, n_estimators=50; total time=   0.1s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=3, n_estimators=50; total time=   0.1s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=3, n_estimators=50; total time=   0.1s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=3, n_estimators=50; total time=   0.2s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=4, n_estimators=10; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=4, n_estimators=10; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=4, n_estimators=10; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=4, n_estimators=10; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=4, n_estimators=10; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=4, n_estimators=20; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=4, n_estimators=20; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=4, n_estimators=20; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=4, n_estimators=20; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=4, n_estimators=20; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=4, n_estimators=30; total time=   0.1s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=4, n_estimators=30; total time=   0.1s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=4, n_estimators=30; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=4, n_estimators=30; total time=   0.1s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=4, n_estimators=30; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=4, n_estimators=40; total time=   0.1s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=4, n_estimators=40; total time=   0.1s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=4, n_estimators=40; total time=   0.1s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=4, n_estimators=40; total time=   0.1s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=4, n_estimators=40; total time=   0.1s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=4, n_estimators=50; total time=   0.1s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=4, n_estimators=50; total time=   0.1s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=4, n_estimators=50; total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END algorithm=SAMME, learning_rate=4, n_estimators=50; total time=   0.1s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=4, n_estimators=50; total time=   0.1s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=5, n_estimators=10; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=5, n_estimators=10; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=5, n_estimators=10; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=5, n_estimators=10; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=5, n_estimators=10; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=5, n_estimators=20; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=5, n_estimators=20; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=5, n_estimators=20; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=5, n_estimators=20; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=5, n_estimators=20; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=5, n_estimators=30; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=5, n_estimators=30; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=5, n_estimators=30; total time=   0.1s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=5, n_estimators=30; total time=   0.1s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=5, n_estimators=30; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=5, n_estimators=40; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=5, n_estimators=40; total time=   0.1s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=5, n_estimators=40; total time=   0.1s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=5, n_estimators=40; total time=   0.1s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=5, n_estimators=40; total time=   0.1s\n",
      "[CV 1/5] END algorithm=SAMME, learning_rate=5, n_estimators=50; total time=   0.1s\n",
      "[CV 2/5] END algorithm=SAMME, learning_rate=5, n_estimators=50; total time=   0.1s\n",
      "[CV 3/5] END algorithm=SAMME, learning_rate=5, n_estimators=50; total time=   0.1s\n",
      "[CV 4/5] END algorithm=SAMME, learning_rate=5, n_estimators=50; total time=   0.1s\n",
      "[CV 5/5] END algorithm=SAMME, learning_rate=5, n_estimators=50; total time=   0.1s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=10; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=10; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=10; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=10; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=10; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=20; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=20; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=20; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=20; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=20; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=30; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=30; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=30; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=30; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=30; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=40; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=40; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=40; total time=   0.1s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=40; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=40; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=50; total time=   0.1s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=50; total time=   0.1s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=50; total time=   0.1s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=50; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=1, n_estimators=50; total time=   0.1s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=2, n_estimators=10; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=2, n_estimators=10; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=2, n_estimators=10; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=2, n_estimators=10; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=2, n_estimators=10; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=2, n_estimators=20; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=2, n_estimators=20; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=2, n_estimators=20; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=2, n_estimators=20; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=2, n_estimators=20; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=2, n_estimators=30; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=2, n_estimators=30; total time=   0.1s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=2, n_estimators=30; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=2, n_estimators=30; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=2, n_estimators=30; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=2, n_estimators=40; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=2, n_estimators=40; total time=   0.1s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=2, n_estimators=40; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=2, n_estimators=40; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=2, n_estimators=40; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=2, n_estimators=50; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=2, n_estimators=50; total time=   0.1s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=2, n_estimators=50; total time=   0.1s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=2, n_estimators=50; total time=   0.1s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=2, n_estimators=50; total time=   0.1s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=3, n_estimators=10; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=3, n_estimators=10; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=3, n_estimators=10; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=3, n_estimators=10; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=3, n_estimators=10; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=3, n_estimators=20; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=3, n_estimators=20; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=3, n_estimators=20; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=3, n_estimators=20; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=3, n_estimators=20; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=3, n_estimators=30; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=3, n_estimators=30; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=3, n_estimators=30; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=3, n_estimators=30; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=3, n_estimators=30; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=3, n_estimators=40; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=3, n_estimators=40; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=3, n_estimators=40; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=3, n_estimators=40; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=3, n_estimators=40; total time=   0.1s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=3, n_estimators=50; total time=   0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=3, n_estimators=50; total time=   0.1s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=3, n_estimators=50; total time=   0.2s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=3, n_estimators=50; total time=   0.2s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=3, n_estimators=50; total time=   0.1s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=4, n_estimators=10; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=4, n_estimators=10; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=4, n_estimators=10; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=4, n_estimators=10; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=4, n_estimators=10; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=4, n_estimators=20; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=4, n_estimators=20; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=4, n_estimators=20; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=4, n_estimators=20; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=4, n_estimators=20; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=4, n_estimators=30; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=4, n_estimators=30; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=4, n_estimators=30; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=4, n_estimators=30; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=4, n_estimators=30; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=4, n_estimators=40; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=4, n_estimators=40; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=4, n_estimators=40; total time=   0.1s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=4, n_estimators=40; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=4, n_estimators=40; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=4, n_estimators=50; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=4, n_estimators=50; total time=   0.1s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=4, n_estimators=50; total time=   0.1s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=4, n_estimators=50; total time=   0.1s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=4, n_estimators=50; total time=   0.1s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=5, n_estimators=10; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=5, n_estimators=10; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=5, n_estimators=10; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=5, n_estimators=10; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=5, n_estimators=10; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=5, n_estimators=20; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=5, n_estimators=20; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=5, n_estimators=20; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=5, n_estimators=20; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=5, n_estimators=20; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=5, n_estimators=30; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=5, n_estimators=30; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=5, n_estimators=30; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=5, n_estimators=30; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=5, n_estimators=30; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=5, n_estimators=40; total time=   0.0s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=5, n_estimators=40; total time=   0.0s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=5, n_estimators=40; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=5, n_estimators=40; total time=   0.0s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=5, n_estimators=40; total time=   0.0s\n",
      "[CV 1/5] END algorithm=SAMME.R, learning_rate=5, n_estimators=50; total time=   0.1s\n",
      "[CV 2/5] END algorithm=SAMME.R, learning_rate=5, n_estimators=50; total time=   0.1s\n",
      "[CV 3/5] END algorithm=SAMME.R, learning_rate=5, n_estimators=50; total time=   0.0s\n",
      "[CV 4/5] END algorithm=SAMME.R, learning_rate=5, n_estimators=50; total time=   0.1s\n",
      "[CV 5/5] END algorithm=SAMME.R, learning_rate=5, n_estimators=50; total time=   0.1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=AdaBoostClassifier(),\n",
       "             param_grid={'algorithm': ['SAMME', 'SAMME.R'],\n",
       "                         'learning_rate': [1, 2, 3, 4, 5],\n",
       "                         'n_estimators': [10, 20, 30, 40, 50]},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OWjvFU3S5zlq",
    "outputId": "b106f956-311a-4ecc-c936-497d533bbcbd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'SAMME.R', 'learning_rate': 2, 'n_estimators': 50}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "UxJe0ZJl54ks"
   },
   "outputs": [],
   "source": [
    "model4=grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "rJD7iURh57Es"
   },
   "outputs": [],
   "source": [
    "y_pred4= model4.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FtUgSp_I59BM",
    "outputId": "fd85e220-8095-463d-f155-eec030b8bada"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FELq-eni6HXl"
   },
   "source": [
    "## 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxNMdMoy6Joz"
   },
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XIPCL9aA6Kvb",
    "outputId": "62d642ca-7fa6-452e-f871-ec809e63d508"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(learning_rate=1.0, max_depth=1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1)\n",
    "model5.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "iNr2oVhv6NWl"
   },
   "outputs": [],
   "source": [
    "y_pred5= model5.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CP09dl5Y6QT0",
    "outputId": "67c41fe7-241c-466a-c0c0-45ca9c1974db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PA_mxBfN6Wek"
   },
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "2gnyZkUk6Xlu"
   },
   "outputs": [],
   "source": [
    "param_grid={'loss':[ 'deviance','exponential'],'learning_rate':[1,2,3,4,5],'criterion':['friedman_mse', 'mse', 'mae'],'max_features':['auto', 'sqrt', 'log2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "-AHBkBVr6beV"
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(GradientBoostingClassifier(),param_grid,refit=True,verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "[CV 1/5] END criterion=friedman_mse, learning_rate=1, loss=deviance, max_features=auto; total time=   2.6s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=1, loss=deviance, max_features=auto; total time=   2.5s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=1, loss=deviance, max_features=auto; total time=   2.6s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=1, loss=deviance, max_features=auto; total time=   2.6s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=1, loss=deviance, max_features=auto; total time=   2.5s\n",
      "[CV 1/5] END criterion=friedman_mse, learning_rate=1, loss=deviance, max_features=sqrt; total time=   1.6s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=1, loss=deviance, max_features=sqrt; total time=   1.5s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=1, loss=deviance, max_features=sqrt; total time=   1.6s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=1, loss=deviance, max_features=sqrt; total time=   1.7s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=1, loss=deviance, max_features=sqrt; total time=   1.5s\n",
      "[CV 1/5] END criterion=friedman_mse, learning_rate=1, loss=deviance, max_features=log2; total time=   1.4s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=1, loss=deviance, max_features=log2; total time=   1.6s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=1, loss=deviance, max_features=log2; total time=   1.7s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=1, loss=deviance, max_features=log2; total time=   1.5s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=1, loss=deviance, max_features=log2; total time=   1.6s\n",
      "[CV 1/5] END criterion=friedman_mse, learning_rate=1, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=1, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=1, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=1, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=1, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 1/5] END criterion=friedman_mse, learning_rate=1, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=1, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=1, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=1, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=1, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 1/5] END criterion=friedman_mse, learning_rate=1, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=1, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=1, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=1, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=1, loss=exponential, max_features=log2; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END criterion=friedman_mse, learning_rate=2, loss=deviance, max_features=auto; total time=   2.5s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=2, loss=deviance, max_features=auto; total time=   2.4s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=2, loss=deviance, max_features=auto; total time=   2.7s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=2, loss=deviance, max_features=auto; total time=   2.6s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=2, loss=deviance, max_features=auto; total time=   2.4s\n",
      "[CV 1/5] END criterion=friedman_mse, learning_rate=2, loss=deviance, max_features=sqrt; total time=   1.6s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=2, loss=deviance, max_features=sqrt; total time=   1.5s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=2, loss=deviance, max_features=sqrt; total time=   1.6s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=2, loss=deviance, max_features=sqrt; total time=   1.5s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=2, loss=deviance, max_features=sqrt; total time=   1.5s\n",
      "[CV 1/5] END criterion=friedman_mse, learning_rate=2, loss=deviance, max_features=log2; total time=   1.6s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=2, loss=deviance, max_features=log2; total time=   1.6s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=2, loss=deviance, max_features=log2; total time=   1.6s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=2, loss=deviance, max_features=log2; total time=   1.7s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=2, loss=deviance, max_features=log2; total time=   1.6s\n",
      "[CV 1/5] END criterion=friedman_mse, learning_rate=2, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=2, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=2, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=2, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=2, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 1/5] END criterion=friedman_mse, learning_rate=2, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=2, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=2, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=2, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=2, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 1/5] END criterion=friedman_mse, learning_rate=2, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=2, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=2, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=2, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=2, loss=exponential, max_features=log2; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END criterion=friedman_mse, learning_rate=3, loss=deviance, max_features=auto; total time=   2.3s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=3, loss=deviance, max_features=auto; total time=   2.3s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=3, loss=deviance, max_features=auto; total time=   2.6s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=3, loss=deviance, max_features=auto; total time=   2.6s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=3, loss=deviance, max_features=auto; total time=   2.6s\n",
      "[CV 1/5] END criterion=friedman_mse, learning_rate=3, loss=deviance, max_features=sqrt; total time=   1.9s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=3, loss=deviance, max_features=sqrt; total time=   1.6s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=3, loss=deviance, max_features=sqrt; total time=   1.5s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=3, loss=deviance, max_features=sqrt; total time=   1.6s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=3, loss=deviance, max_features=sqrt; total time=   1.6s\n",
      "[CV 1/5] END criterion=friedman_mse, learning_rate=3, loss=deviance, max_features=log2; total time=   1.7s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=3, loss=deviance, max_features=log2; total time=   1.7s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=3, loss=deviance, max_features=log2; total time=   1.6s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=3, loss=deviance, max_features=log2; total time=   1.5s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=3, loss=deviance, max_features=log2; total time=   1.6s\n",
      "[CV 1/5] END criterion=friedman_mse, learning_rate=3, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=3, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=3, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=3, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=3, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 1/5] END criterion=friedman_mse, learning_rate=3, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=3, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=3, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=3, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=3, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 1/5] END criterion=friedman_mse, learning_rate=3, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=3, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=3, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=3, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=3, loss=exponential, max_features=log2; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END criterion=friedman_mse, learning_rate=4, loss=deviance, max_features=auto; total time=   2.5s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=4, loss=deviance, max_features=auto; total time=   2.6s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=4, loss=deviance, max_features=auto; total time=   2.4s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=4, loss=deviance, max_features=auto; total time=   2.6s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=4, loss=deviance, max_features=auto; total time=   2.5s\n",
      "[CV 1/5] END criterion=friedman_mse, learning_rate=4, loss=deviance, max_features=sqrt; total time=   1.5s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=4, loss=deviance, max_features=sqrt; total time=   1.7s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=4, loss=deviance, max_features=sqrt; total time=   1.6s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=4, loss=deviance, max_features=sqrt; total time=   1.6s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=4, loss=deviance, max_features=sqrt; total time=   1.6s\n",
      "[CV 1/5] END criterion=friedman_mse, learning_rate=4, loss=deviance, max_features=log2; total time=   1.5s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=4, loss=deviance, max_features=log2; total time=   1.5s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=4, loss=deviance, max_features=log2; total time=   1.6s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=4, loss=deviance, max_features=log2; total time=   1.5s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=4, loss=deviance, max_features=log2; total time=   1.6s\n",
      "[CV 1/5] END criterion=friedman_mse, learning_rate=4, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=4, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=4, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=4, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=4, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 1/5] END criterion=friedman_mse, learning_rate=4, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=4, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=4, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=4, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=4, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 1/5] END criterion=friedman_mse, learning_rate=4, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=4, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=4, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=4, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=4, loss=exponential, max_features=log2; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END criterion=friedman_mse, learning_rate=5, loss=deviance, max_features=auto; total time=   2.6s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=5, loss=deviance, max_features=auto; total time=   2.4s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=5, loss=deviance, max_features=auto; total time=   2.4s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=5, loss=deviance, max_features=auto; total time=   2.5s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=5, loss=deviance, max_features=auto; total time=   2.6s\n",
      "[CV 1/5] END criterion=friedman_mse, learning_rate=5, loss=deviance, max_features=sqrt; total time=   1.6s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=5, loss=deviance, max_features=sqrt; total time=   1.6s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=5, loss=deviance, max_features=sqrt; total time=   1.7s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=5, loss=deviance, max_features=sqrt; total time=   1.6s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=5, loss=deviance, max_features=sqrt; total time=   1.5s\n",
      "[CV 1/5] END criterion=friedman_mse, learning_rate=5, loss=deviance, max_features=log2; total time=   1.6s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=5, loss=deviance, max_features=log2; total time=   1.6s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=5, loss=deviance, max_features=log2; total time=   1.6s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=5, loss=deviance, max_features=log2; total time=   1.6s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=5, loss=deviance, max_features=log2; total time=   1.6s\n",
      "[CV 1/5] END criterion=friedman_mse, learning_rate=5, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=5, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=5, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=5, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=5, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 1/5] END criterion=friedman_mse, learning_rate=5, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=5, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=5, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=5, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=5, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 1/5] END criterion=friedman_mse, learning_rate=5, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 2/5] END criterion=friedman_mse, learning_rate=5, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 3/5] END criterion=friedman_mse, learning_rate=5, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 4/5] END criterion=friedman_mse, learning_rate=5, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 5/5] END criterion=friedman_mse, learning_rate=5, loss=exponential, max_features=log2; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END criterion=mse, learning_rate=1, loss=deviance, max_features=auto; total time=   2.4s\n",
      "[CV 2/5] END criterion=mse, learning_rate=1, loss=deviance, max_features=auto; total time=   2.6s\n",
      "[CV 3/5] END criterion=mse, learning_rate=1, loss=deviance, max_features=auto; total time=   2.6s\n",
      "[CV 4/5] END criterion=mse, learning_rate=1, loss=deviance, max_features=auto; total time=   2.6s\n",
      "[CV 5/5] END criterion=mse, learning_rate=1, loss=deviance, max_features=auto; total time=   2.5s\n",
      "[CV 1/5] END criterion=mse, learning_rate=1, loss=deviance, max_features=sqrt; total time=   1.6s\n",
      "[CV 2/5] END criterion=mse, learning_rate=1, loss=deviance, max_features=sqrt; total time=   1.6s\n",
      "[CV 3/5] END criterion=mse, learning_rate=1, loss=deviance, max_features=sqrt; total time=   1.4s\n",
      "[CV 4/5] END criterion=mse, learning_rate=1, loss=deviance, max_features=sqrt; total time=   1.6s\n",
      "[CV 5/5] END criterion=mse, learning_rate=1, loss=deviance, max_features=sqrt; total time=   1.4s\n",
      "[CV 1/5] END criterion=mse, learning_rate=1, loss=deviance, max_features=log2; total time=   1.4s\n",
      "[CV 2/5] END criterion=mse, learning_rate=1, loss=deviance, max_features=log2; total time=   1.7s\n",
      "[CV 3/5] END criterion=mse, learning_rate=1, loss=deviance, max_features=log2; total time=   1.5s\n",
      "[CV 4/5] END criterion=mse, learning_rate=1, loss=deviance, max_features=log2; total time=   1.5s\n",
      "[CV 5/5] END criterion=mse, learning_rate=1, loss=deviance, max_features=log2; total time=   1.5s\n",
      "[CV 1/5] END criterion=mse, learning_rate=1, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 2/5] END criterion=mse, learning_rate=1, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 3/5] END criterion=mse, learning_rate=1, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 4/5] END criterion=mse, learning_rate=1, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 5/5] END criterion=mse, learning_rate=1, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 1/5] END criterion=mse, learning_rate=1, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 2/5] END criterion=mse, learning_rate=1, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 3/5] END criterion=mse, learning_rate=1, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 4/5] END criterion=mse, learning_rate=1, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 5/5] END criterion=mse, learning_rate=1, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 1/5] END criterion=mse, learning_rate=1, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 2/5] END criterion=mse, learning_rate=1, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 3/5] END criterion=mse, learning_rate=1, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 4/5] END criterion=mse, learning_rate=1, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 5/5] END criterion=mse, learning_rate=1, loss=exponential, max_features=log2; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END criterion=mse, learning_rate=2, loss=deviance, max_features=auto; total time=   2.7s\n",
      "[CV 2/5] END criterion=mse, learning_rate=2, loss=deviance, max_features=auto; total time=   2.8s\n",
      "[CV 3/5] END criterion=mse, learning_rate=2, loss=deviance, max_features=auto; total time=   2.4s\n",
      "[CV 4/5] END criterion=mse, learning_rate=2, loss=deviance, max_features=auto; total time=   2.5s\n",
      "[CV 5/5] END criterion=mse, learning_rate=2, loss=deviance, max_features=auto; total time=   2.5s\n",
      "[CV 1/5] END criterion=mse, learning_rate=2, loss=deviance, max_features=sqrt; total time=   1.8s\n",
      "[CV 2/5] END criterion=mse, learning_rate=2, loss=deviance, max_features=sqrt; total time=   1.6s\n",
      "[CV 3/5] END criterion=mse, learning_rate=2, loss=deviance, max_features=sqrt; total time=   1.7s\n",
      "[CV 4/5] END criterion=mse, learning_rate=2, loss=deviance, max_features=sqrt; total time=   1.7s\n",
      "[CV 5/5] END criterion=mse, learning_rate=2, loss=deviance, max_features=sqrt; total time=   1.6s\n",
      "[CV 1/5] END criterion=mse, learning_rate=2, loss=deviance, max_features=log2; total time=   1.6s\n",
      "[CV 2/5] END criterion=mse, learning_rate=2, loss=deviance, max_features=log2; total time=   1.6s\n",
      "[CV 3/5] END criterion=mse, learning_rate=2, loss=deviance, max_features=log2; total time=   1.7s\n",
      "[CV 4/5] END criterion=mse, learning_rate=2, loss=deviance, max_features=log2; total time=   1.6s\n",
      "[CV 5/5] END criterion=mse, learning_rate=2, loss=deviance, max_features=log2; total time=   1.6s\n",
      "[CV 1/5] END criterion=mse, learning_rate=2, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 2/5] END criterion=mse, learning_rate=2, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 3/5] END criterion=mse, learning_rate=2, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 4/5] END criterion=mse, learning_rate=2, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 5/5] END criterion=mse, learning_rate=2, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 1/5] END criterion=mse, learning_rate=2, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 2/5] END criterion=mse, learning_rate=2, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 3/5] END criterion=mse, learning_rate=2, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 4/5] END criterion=mse, learning_rate=2, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 5/5] END criterion=mse, learning_rate=2, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 1/5] END criterion=mse, learning_rate=2, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 2/5] END criterion=mse, learning_rate=2, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 3/5] END criterion=mse, learning_rate=2, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 4/5] END criterion=mse, learning_rate=2, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 5/5] END criterion=mse, learning_rate=2, loss=exponential, max_features=log2; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END criterion=mse, learning_rate=3, loss=deviance, max_features=auto; total time=   2.4s\n",
      "[CV 2/5] END criterion=mse, learning_rate=3, loss=deviance, max_features=auto; total time=   2.7s\n",
      "[CV 3/5] END criterion=mse, learning_rate=3, loss=deviance, max_features=auto; total time=   2.4s\n",
      "[CV 4/5] END criterion=mse, learning_rate=3, loss=deviance, max_features=auto; total time=   2.5s\n",
      "[CV 5/5] END criterion=mse, learning_rate=3, loss=deviance, max_features=auto; total time=   2.5s\n",
      "[CV 1/5] END criterion=mse, learning_rate=3, loss=deviance, max_features=sqrt; total time=   1.6s\n",
      "[CV 2/5] END criterion=mse, learning_rate=3, loss=deviance, max_features=sqrt; total time=   1.6s\n",
      "[CV 3/5] END criterion=mse, learning_rate=3, loss=deviance, max_features=sqrt; total time=   1.5s\n",
      "[CV 4/5] END criterion=mse, learning_rate=3, loss=deviance, max_features=sqrt; total time=   1.6s\n",
      "[CV 5/5] END criterion=mse, learning_rate=3, loss=deviance, max_features=sqrt; total time=   1.6s\n",
      "[CV 1/5] END criterion=mse, learning_rate=3, loss=deviance, max_features=log2; total time=   1.5s\n",
      "[CV 2/5] END criterion=mse, learning_rate=3, loss=deviance, max_features=log2; total time=   1.6s\n",
      "[CV 3/5] END criterion=mse, learning_rate=3, loss=deviance, max_features=log2; total time=   1.6s\n",
      "[CV 4/5] END criterion=mse, learning_rate=3, loss=deviance, max_features=log2; total time=   1.6s\n",
      "[CV 5/5] END criterion=mse, learning_rate=3, loss=deviance, max_features=log2; total time=   1.7s\n",
      "[CV 1/5] END criterion=mse, learning_rate=3, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 2/5] END criterion=mse, learning_rate=3, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 3/5] END criterion=mse, learning_rate=3, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 4/5] END criterion=mse, learning_rate=3, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 5/5] END criterion=mse, learning_rate=3, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 1/5] END criterion=mse, learning_rate=3, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 2/5] END criterion=mse, learning_rate=3, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 3/5] END criterion=mse, learning_rate=3, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 4/5] END criterion=mse, learning_rate=3, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 5/5] END criterion=mse, learning_rate=3, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 1/5] END criterion=mse, learning_rate=3, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 2/5] END criterion=mse, learning_rate=3, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 3/5] END criterion=mse, learning_rate=3, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 4/5] END criterion=mse, learning_rate=3, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 5/5] END criterion=mse, learning_rate=3, loss=exponential, max_features=log2; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END criterion=mse, learning_rate=4, loss=deviance, max_features=auto; total time=   2.5s\n",
      "[CV 2/5] END criterion=mse, learning_rate=4, loss=deviance, max_features=auto; total time=   2.7s\n",
      "[CV 3/5] END criterion=mse, learning_rate=4, loss=deviance, max_features=auto; total time=   2.9s\n",
      "[CV 4/5] END criterion=mse, learning_rate=4, loss=deviance, max_features=auto; total time=   2.8s\n",
      "[CV 5/5] END criterion=mse, learning_rate=4, loss=deviance, max_features=auto; total time=   2.7s\n",
      "[CV 1/5] END criterion=mse, learning_rate=4, loss=deviance, max_features=sqrt; total time=   1.6s\n",
      "[CV 2/5] END criterion=mse, learning_rate=4, loss=deviance, max_features=sqrt; total time=   2.1s\n",
      "[CV 3/5] END criterion=mse, learning_rate=4, loss=deviance, max_features=sqrt; total time=   1.6s\n",
      "[CV 4/5] END criterion=mse, learning_rate=4, loss=deviance, max_features=sqrt; total time=   1.6s\n",
      "[CV 5/5] END criterion=mse, learning_rate=4, loss=deviance, max_features=sqrt; total time=   1.7s\n",
      "[CV 1/5] END criterion=mse, learning_rate=4, loss=deviance, max_features=log2; total time=   1.6s\n",
      "[CV 2/5] END criterion=mse, learning_rate=4, loss=deviance, max_features=log2; total time=   1.6s\n",
      "[CV 3/5] END criterion=mse, learning_rate=4, loss=deviance, max_features=log2; total time=   1.6s\n",
      "[CV 4/5] END criterion=mse, learning_rate=4, loss=deviance, max_features=log2; total time=   1.6s\n",
      "[CV 5/5] END criterion=mse, learning_rate=4, loss=deviance, max_features=log2; total time=   1.5s\n",
      "[CV 1/5] END criterion=mse, learning_rate=4, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 2/5] END criterion=mse, learning_rate=4, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 3/5] END criterion=mse, learning_rate=4, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 4/5] END criterion=mse, learning_rate=4, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 5/5] END criterion=mse, learning_rate=4, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 1/5] END criterion=mse, learning_rate=4, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 2/5] END criterion=mse, learning_rate=4, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 3/5] END criterion=mse, learning_rate=4, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 4/5] END criterion=mse, learning_rate=4, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 5/5] END criterion=mse, learning_rate=4, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 1/5] END criterion=mse, learning_rate=4, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 2/5] END criterion=mse, learning_rate=4, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 3/5] END criterion=mse, learning_rate=4, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 4/5] END criterion=mse, learning_rate=4, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 5/5] END criterion=mse, learning_rate=4, loss=exponential, max_features=log2; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END criterion=mse, learning_rate=5, loss=deviance, max_features=auto; total time=   2.5s\n",
      "[CV 2/5] END criterion=mse, learning_rate=5, loss=deviance, max_features=auto; total time=   2.4s\n",
      "[CV 3/5] END criterion=mse, learning_rate=5, loss=deviance, max_features=auto; total time=   2.6s\n",
      "[CV 4/5] END criterion=mse, learning_rate=5, loss=deviance, max_features=auto; total time=   2.6s\n",
      "[CV 5/5] END criterion=mse, learning_rate=5, loss=deviance, max_features=auto; total time=   2.6s\n",
      "[CV 1/5] END criterion=mse, learning_rate=5, loss=deviance, max_features=sqrt; total time=   1.6s\n",
      "[CV 2/5] END criterion=mse, learning_rate=5, loss=deviance, max_features=sqrt; total time=   1.6s\n",
      "[CV 3/5] END criterion=mse, learning_rate=5, loss=deviance, max_features=sqrt; total time=   2.2s\n",
      "[CV 4/5] END criterion=mse, learning_rate=5, loss=deviance, max_features=sqrt; total time=   2.1s\n",
      "[CV 5/5] END criterion=mse, learning_rate=5, loss=deviance, max_features=sqrt; total time=   1.9s\n",
      "[CV 1/5] END criterion=mse, learning_rate=5, loss=deviance, max_features=log2; total time=   2.0s\n",
      "[CV 2/5] END criterion=mse, learning_rate=5, loss=deviance, max_features=log2; total time=   1.7s\n",
      "[CV 3/5] END criterion=mse, learning_rate=5, loss=deviance, max_features=log2; total time=   1.8s\n",
      "[CV 4/5] END criterion=mse, learning_rate=5, loss=deviance, max_features=log2; total time=   1.6s\n",
      "[CV 5/5] END criterion=mse, learning_rate=5, loss=deviance, max_features=log2; total time=   1.9s\n",
      "[CV 1/5] END criterion=mse, learning_rate=5, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 2/5] END criterion=mse, learning_rate=5, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 3/5] END criterion=mse, learning_rate=5, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 4/5] END criterion=mse, learning_rate=5, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 5/5] END criterion=mse, learning_rate=5, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 1/5] END criterion=mse, learning_rate=5, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 2/5] END criterion=mse, learning_rate=5, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 3/5] END criterion=mse, learning_rate=5, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 4/5] END criterion=mse, learning_rate=5, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 5/5] END criterion=mse, learning_rate=5, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 1/5] END criterion=mse, learning_rate=5, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 2/5] END criterion=mse, learning_rate=5, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 3/5] END criterion=mse, learning_rate=5, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 4/5] END criterion=mse, learning_rate=5, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 5/5] END criterion=mse, learning_rate=5, loss=exponential, max_features=log2; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END criterion=mae, learning_rate=1, loss=deviance, max_features=auto; total time=  17.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END criterion=mae, learning_rate=1, loss=deviance, max_features=auto; total time=  17.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END criterion=mae, learning_rate=1, loss=deviance, max_features=auto; total time=  18.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END criterion=mae, learning_rate=1, loss=deviance, max_features=auto; total time=  20.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END criterion=mae, learning_rate=1, loss=deviance, max_features=auto; total time=  16.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END criterion=mae, learning_rate=1, loss=deviance, max_features=sqrt; total time=   5.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END criterion=mae, learning_rate=1, loss=deviance, max_features=sqrt; total time=   5.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END criterion=mae, learning_rate=1, loss=deviance, max_features=sqrt; total time=   5.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END criterion=mae, learning_rate=1, loss=deviance, max_features=sqrt; total time=   5.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END criterion=mae, learning_rate=1, loss=deviance, max_features=sqrt; total time=   4.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END criterion=mae, learning_rate=1, loss=deviance, max_features=log2; total time=   5.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END criterion=mae, learning_rate=1, loss=deviance, max_features=log2; total time=   5.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END criterion=mae, learning_rate=1, loss=deviance, max_features=log2; total time=   5.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END criterion=mae, learning_rate=1, loss=deviance, max_features=log2; total time=   5.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END criterion=mae, learning_rate=1, loss=deviance, max_features=log2; total time=   5.9s\n",
      "[CV 1/5] END criterion=mae, learning_rate=1, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 2/5] END criterion=mae, learning_rate=1, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 3/5] END criterion=mae, learning_rate=1, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 4/5] END criterion=mae, learning_rate=1, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 5/5] END criterion=mae, learning_rate=1, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 1/5] END criterion=mae, learning_rate=1, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 2/5] END criterion=mae, learning_rate=1, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 3/5] END criterion=mae, learning_rate=1, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 4/5] END criterion=mae, learning_rate=1, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 5/5] END criterion=mae, learning_rate=1, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 1/5] END criterion=mae, learning_rate=1, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 2/5] END criterion=mae, learning_rate=1, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 3/5] END criterion=mae, learning_rate=1, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 4/5] END criterion=mae, learning_rate=1, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 5/5] END criterion=mae, learning_rate=1, loss=exponential, max_features=log2; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END criterion=mae, learning_rate=2, loss=deviance, max_features=auto; total time=  16.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END criterion=mae, learning_rate=2, loss=deviance, max_features=auto; total time=  17.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END criterion=mae, learning_rate=2, loss=deviance, max_features=auto; total time=  17.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END criterion=mae, learning_rate=2, loss=deviance, max_features=auto; total time=  17.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END criterion=mae, learning_rate=2, loss=deviance, max_features=auto; total time=  17.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END criterion=mae, learning_rate=2, loss=deviance, max_features=sqrt; total time=   4.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END criterion=mae, learning_rate=2, loss=deviance, max_features=sqrt; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END criterion=mae, learning_rate=2, loss=deviance, max_features=sqrt; total time=   4.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END criterion=mae, learning_rate=2, loss=deviance, max_features=sqrt; total time=   5.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END criterion=mae, learning_rate=2, loss=deviance, max_features=sqrt; total time=   4.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END criterion=mae, learning_rate=2, loss=deviance, max_features=log2; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END criterion=mae, learning_rate=2, loss=deviance, max_features=log2; total time=   5.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END criterion=mae, learning_rate=2, loss=deviance, max_features=log2; total time=   5.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END criterion=mae, learning_rate=2, loss=deviance, max_features=log2; total time=   5.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END criterion=mae, learning_rate=2, loss=deviance, max_features=log2; total time=   5.2s\n",
      "[CV 1/5] END criterion=mae, learning_rate=2, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 2/5] END criterion=mae, learning_rate=2, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 3/5] END criterion=mae, learning_rate=2, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 4/5] END criterion=mae, learning_rate=2, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 5/5] END criterion=mae, learning_rate=2, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 1/5] END criterion=mae, learning_rate=2, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 2/5] END criterion=mae, learning_rate=2, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 3/5] END criterion=mae, learning_rate=2, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 4/5] END criterion=mae, learning_rate=2, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 5/5] END criterion=mae, learning_rate=2, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 1/5] END criterion=mae, learning_rate=2, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 2/5] END criterion=mae, learning_rate=2, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 3/5] END criterion=mae, learning_rate=2, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 4/5] END criterion=mae, learning_rate=2, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 5/5] END criterion=mae, learning_rate=2, loss=exponential, max_features=log2; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END criterion=mae, learning_rate=3, loss=deviance, max_features=auto; total time=  16.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END criterion=mae, learning_rate=3, loss=deviance, max_features=auto; total time=  17.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END criterion=mae, learning_rate=3, loss=deviance, max_features=auto; total time=  17.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END criterion=mae, learning_rate=3, loss=deviance, max_features=auto; total time=  17.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END criterion=mae, learning_rate=3, loss=deviance, max_features=auto; total time=  17.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END criterion=mae, learning_rate=3, loss=deviance, max_features=sqrt; total time=   4.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END criterion=mae, learning_rate=3, loss=deviance, max_features=sqrt; total time=   4.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END criterion=mae, learning_rate=3, loss=deviance, max_features=sqrt; total time=   4.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END criterion=mae, learning_rate=3, loss=deviance, max_features=sqrt; total time=   5.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END criterion=mae, learning_rate=3, loss=deviance, max_features=sqrt; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END criterion=mae, learning_rate=3, loss=deviance, max_features=log2; total time=   4.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END criterion=mae, learning_rate=3, loss=deviance, max_features=log2; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END criterion=mae, learning_rate=3, loss=deviance, max_features=log2; total time=   5.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END criterion=mae, learning_rate=3, loss=deviance, max_features=log2; total time=   5.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END criterion=mae, learning_rate=3, loss=deviance, max_features=log2; total time=   5.1s\n",
      "[CV 1/5] END criterion=mae, learning_rate=3, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 2/5] END criterion=mae, learning_rate=3, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 3/5] END criterion=mae, learning_rate=3, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 4/5] END criterion=mae, learning_rate=3, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 5/5] END criterion=mae, learning_rate=3, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 1/5] END criterion=mae, learning_rate=3, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 2/5] END criterion=mae, learning_rate=3, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 3/5] END criterion=mae, learning_rate=3, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 4/5] END criterion=mae, learning_rate=3, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 5/5] END criterion=mae, learning_rate=3, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 1/5] END criterion=mae, learning_rate=3, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 2/5] END criterion=mae, learning_rate=3, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 3/5] END criterion=mae, learning_rate=3, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 4/5] END criterion=mae, learning_rate=3, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 5/5] END criterion=mae, learning_rate=3, loss=exponential, max_features=log2; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END criterion=mae, learning_rate=4, loss=deviance, max_features=auto; total time=  17.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END criterion=mae, learning_rate=4, loss=deviance, max_features=auto; total time=  16.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END criterion=mae, learning_rate=4, loss=deviance, max_features=auto; total time=  16.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END criterion=mae, learning_rate=4, loss=deviance, max_features=auto; total time=  17.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END criterion=mae, learning_rate=4, loss=deviance, max_features=auto; total time=  17.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END criterion=mae, learning_rate=4, loss=deviance, max_features=sqrt; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END criterion=mae, learning_rate=4, loss=deviance, max_features=sqrt; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END criterion=mae, learning_rate=4, loss=deviance, max_features=sqrt; total time=   4.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END criterion=mae, learning_rate=4, loss=deviance, max_features=sqrt; total time=   4.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END criterion=mae, learning_rate=4, loss=deviance, max_features=sqrt; total time=   4.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END criterion=mae, learning_rate=4, loss=deviance, max_features=log2; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END criterion=mae, learning_rate=4, loss=deviance, max_features=log2; total time=   5.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END criterion=mae, learning_rate=4, loss=deviance, max_features=log2; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END criterion=mae, learning_rate=4, loss=deviance, max_features=log2; total time=   5.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END criterion=mae, learning_rate=4, loss=deviance, max_features=log2; total time=   4.9s\n",
      "[CV 1/5] END criterion=mae, learning_rate=4, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 2/5] END criterion=mae, learning_rate=4, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 3/5] END criterion=mae, learning_rate=4, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 4/5] END criterion=mae, learning_rate=4, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 5/5] END criterion=mae, learning_rate=4, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 1/5] END criterion=mae, learning_rate=4, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 2/5] END criterion=mae, learning_rate=4, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 3/5] END criterion=mae, learning_rate=4, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 4/5] END criterion=mae, learning_rate=4, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 5/5] END criterion=mae, learning_rate=4, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 1/5] END criterion=mae, learning_rate=4, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 2/5] END criterion=mae, learning_rate=4, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 3/5] END criterion=mae, learning_rate=4, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 4/5] END criterion=mae, learning_rate=4, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 5/5] END criterion=mae, learning_rate=4, loss=exponential, max_features=log2; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END criterion=mae, learning_rate=5, loss=deviance, max_features=auto; total time=  17.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END criterion=mae, learning_rate=5, loss=deviance, max_features=auto; total time=  18.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END criterion=mae, learning_rate=5, loss=deviance, max_features=auto; total time=  17.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END criterion=mae, learning_rate=5, loss=deviance, max_features=auto; total time=  17.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END criterion=mae, learning_rate=5, loss=deviance, max_features=auto; total time=  17.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END criterion=mae, learning_rate=5, loss=deviance, max_features=sqrt; total time=   4.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END criterion=mae, learning_rate=5, loss=deviance, max_features=sqrt; total time=   4.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END criterion=mae, learning_rate=5, loss=deviance, max_features=sqrt; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END criterion=mae, learning_rate=5, loss=deviance, max_features=sqrt; total time=   4.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END criterion=mae, learning_rate=5, loss=deviance, max_features=sqrt; total time=   4.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END criterion=mae, learning_rate=5, loss=deviance, max_features=log2; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END criterion=mae, learning_rate=5, loss=deviance, max_features=log2; total time=   4.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END criterion=mae, learning_rate=5, loss=deviance, max_features=log2; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END criterion=mae, learning_rate=5, loss=deviance, max_features=log2; total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END criterion=mae, learning_rate=5, loss=deviance, max_features=log2; total time=   4.9s\n",
      "[CV 1/5] END criterion=mae, learning_rate=5, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 2/5] END criterion=mae, learning_rate=5, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 3/5] END criterion=mae, learning_rate=5, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 4/5] END criterion=mae, learning_rate=5, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 5/5] END criterion=mae, learning_rate=5, loss=exponential, max_features=auto; total time=   0.0s\n",
      "[CV 1/5] END criterion=mae, learning_rate=5, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 2/5] END criterion=mae, learning_rate=5, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 3/5] END criterion=mae, learning_rate=5, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 4/5] END criterion=mae, learning_rate=5, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 5/5] END criterion=mae, learning_rate=5, loss=exponential, max_features=sqrt; total time=   0.0s\n",
      "[CV 1/5] END criterion=mae, learning_rate=5, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 2/5] END criterion=mae, learning_rate=5, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 3/5] END criterion=mae, learning_rate=5, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 4/5] END criterion=mae, learning_rate=5, loss=exponential, max_features=log2; total time=   0.0s\n",
      "[CV 5/5] END criterion=mae, learning_rate=5, loss=exponential, max_features=log2; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1122: FutureWarning: criterion='mae' was deprecated in version 0.24 and will be removed in version 1.1 (renaming of 0.26). Use criterion='friedman_mse' or 'mse' instead, as trees should use a least-square criterion in Gradient Boosting.\n",
      "  \"Boosting.\", FutureWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 448, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 249, in _check_params\n",
      "    self.loss_ = loss_class(self.n_classes_)\n",
      "  File \"C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py\", line 776, in __init__\n",
      "    .format(self.__class__.__name__, n_classes))\n",
      "ValueError: ExponentialLoss requires 2 classes; got 10 class(es)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.5425 0.38   0.195     nan    nan    nan 0.1325 0.13   0.1925    nan\n",
      "    nan    nan 0.1575 0.19   0.215     nan    nan    nan 0.2125 0.145\n",
      " 0.2225    nan    nan    nan 0.1475 0.1175 0.1775    nan    nan    nan\n",
      " 0.66   0.355  0.395     nan    nan    nan 0.18   0.22   0.1625    nan\n",
      "    nan    nan 0.215  0.0975 0.1925    nan    nan    nan 0.2    0.13\n",
      " 0.2525    nan    nan    nan 0.165  0.2175 0.1325    nan    nan    nan\n",
      " 0.18   0.44   0.22      nan    nan    nan 0.0925 0.13   0.095     nan\n",
      "    nan    nan 0.1025 0.145  0.095     nan    nan    nan 0.0875 0.1625\n",
      " 0.08      nan    nan    nan 0.17   0.145  0.1125    nan    nan    nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=GradientBoostingClassifier(),\n",
       "             param_grid={'criterion': ['friedman_mse', 'mse', 'mae'],\n",
       "                         'learning_rate': [1, 2, 3, 4, 5],\n",
       "                         'loss': ['deviance', 'exponential'],\n",
       "                         'max_features': ['auto', 'sqrt', 'log2']},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xOdgHFbd6eof",
    "outputId": "6189324f-5669-4091-a326-6c440a1186e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'mse',\n",
       " 'learning_rate': 1,\n",
       " 'loss': 'deviance',\n",
       " 'max_features': 'auto'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "SFjTdUJE6gxg"
   },
   "outputs": [],
   "source": [
    "model6=grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "-Kc8Xazn7YYI"
   },
   "outputs": [],
   "source": [
    "y_pred6=model6.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wdagc2nG7ZLI",
    "outputId": "779026ca-47a8-4537-aa08-77df9152e760"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RfTJ1K_7gQ_"
   },
   "source": [
    "# Bagging Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HpeW9rD7rMu"
   },
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "9u1UYs__7vPz"
   },
   "outputs": [],
   "source": [
    "seed = 10\n",
    "kfold = model_selection.KFold(n_splits = 3,shuffle=True,\n",
    "                       random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "oaXRI0s97z4o"
   },
   "outputs": [],
   "source": [
    "base_cls = DecisionTreeClassifier()\n",
    "num_trees = 500\n",
    "model7 = BaggingClassifier(base_estimator = base_cls,\n",
    "                          n_estimators = num_trees,\n",
    "                          random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2r8zysto714c",
    "outputId": "389abea0-82a8-4252-b894-27a0cf2981d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7475404930236037"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_selection.cross_val_score(model7, x_train, y_train, cv = kfold).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sMxWy6d875V6",
    "outputId": "db502958-b4ff-4ce4-cfd3-01b4556fa42e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=500,\n",
       "                  random_state=10)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model7.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XC9iNQ8c78Y7",
    "outputId": "980a6e01-83fc-4f17-d3b9-c7de356bc66d"
   },
   "outputs": [],
   "source": [
    "y_pred7=model7.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CPwQ2lU8HGk"
   },
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "AETH4-vn8JKH"
   },
   "outputs": [],
   "source": [
    "seed = 9\n",
    "kfold = model_selection.KFold(n_splits = 3,shuffle=True,\n",
    "                       random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "LP-GIALL8Lew"
   },
   "outputs": [],
   "source": [
    "base_cls = SVC()\n",
    "num_trees = 500\n",
    "model8 = BaggingClassifier(base_estimator = base_cls,\n",
    "                          n_estimators = num_trees,\n",
    "                          random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZKL_n-y-8Pq7",
    "outputId": "52bc4734-1df3-4241-c945-202f3ed380a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6926083866382373"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_selection.cross_val_score(model8, x_train, y_train, cv = kfold).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u1u5NMoQ8S_A",
    "outputId": "85008bd9-525e-434a-b2c0-12e5972244f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=SVC(), n_estimators=500, random_state=9)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model8.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "UdSVtLpP8U3L"
   },
   "outputs": [],
   "source": [
    "y_pred8=model8.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PEbv8W1v8YIi",
    "outputId": "735bea22-88aa-4d4f-f2bd-f9588cc75a98"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pab3Lc-N8klj"
   },
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkYkFyMa8m5a"
   },
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "Szoc8Lnb8pJ4"
   },
   "outputs": [],
   "source": [
    "model9 = RandomForestClassifier(max_depth=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0X9_98Z78w2C",
    "outputId": "66cce31e-ede4-4341-d7b2-9bcd349de296"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=2, random_state=0)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model9.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M0i01GkN80Ox",
    "outputId": "1aa65790-d9b4-4dac-ca6c-667d25fb2f63"
   },
   "outputs": [],
   "source": [
    "y_pred9=model9.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDYmR6XA88sV"
   },
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "t77qu2Vf9Mou"
   },
   "outputs": [],
   "source": [
    "param_grid={'n_estimators':[100,200,300,400],'criterion':['gini', 'entropy'],'max_depth':[2,3,4,5],'max_features':['auto','sqrt','log2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "7kBB9KhR9PPg"
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(RandomForestClassifier(),param_grid,refit=True,verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3olUyMOV9RUu",
    "outputId": "a3235f77-4f33-4272-a270-b4acfc1d41f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=100; total time=   0.2s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=100; total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=100; total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=100; total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=100; total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=200; total time=   0.3s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=200; total time=   0.3s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=200; total time=   0.4s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=200; total time=   0.3s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=200; total time=   0.4s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=300; total time=   0.5s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=300; total time=   0.5s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=300; total time=   0.5s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=300; total time=   0.5s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=300; total time=   0.5s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=400; total time=   0.8s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=400; total time=   0.7s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=400; total time=   0.8s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=400; total time=   0.7s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=400; total time=   0.7s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=200; total time=   0.3s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=200; total time=   0.3s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=200; total time=   0.3s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=200; total time=   0.3s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=200; total time=   0.3s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=300; total time=   0.5s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=300; total time=   0.5s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=300; total time=   0.5s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=300; total time=   0.5s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=300; total time=   0.5s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=400; total time=   0.7s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=400; total time=   0.7s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=400; total time=   0.8s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=400; total time=   0.7s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=400; total time=   0.7s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=100; total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=100; total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=100; total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=100; total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=100; total time=   0.2s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=200; total time=   0.3s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=200; total time=   0.3s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=200; total time=   0.3s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=200; total time=   0.3s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=200; total time=   0.3s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=300; total time=   0.5s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=300; total time=   0.5s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=300; total time=   0.6s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=300; total time=   0.6s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=300; total time=   0.5s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=400; total time=   0.7s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=400; total time=   0.7s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=400; total time=   0.7s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=400; total time=   0.8s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=400; total time=   0.7s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=100; total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=100; total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=100; total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=100; total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=100; total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=200; total time=   0.3s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=200; total time=   0.4s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=200; total time=   0.4s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=200; total time=   0.3s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=200; total time=   0.3s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=300; total time=   0.5s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=300; total time=   0.5s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=300; total time=   0.6s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=300; total time=   0.6s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=300; total time=   0.6s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=400; total time=   1.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=400; total time=   1.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=400; total time=   0.8s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=400; total time=   0.8s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=400; total time=   0.7s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=100; total time=   0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=100; total time=   0.2s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=200; total time=   0.4s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=200; total time=   0.4s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=200; total time=   0.3s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=200; total time=   0.3s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=200; total time=   0.4s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=300; total time=   0.6s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=300; total time=   0.5s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=300; total time=   0.6s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=300; total time=   0.6s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=300; total time=   0.5s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=400; total time=   1.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=400; total time=   1.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=400; total time=   1.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=400; total time=   0.8s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=400; total time=   0.8s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=100; total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=100; total time=   0.2s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=100; total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=100; total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=100; total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=200; total time=   0.4s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=200; total time=   0.3s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=200; total time=   0.4s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=200; total time=   0.3s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=200; total time=   0.3s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=300; total time=   0.6s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=300; total time=   0.7s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=300; total time=   0.6s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=300; total time=   0.6s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=300; total time=   0.6s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=400; total time=   0.7s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=400; total time=   0.8s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=400; total time=   0.8s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=400; total time=   0.7s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=400; total time=   0.8s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=100; total time=   0.2s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=100; total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=100; total time=   0.2s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=100; total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=100; total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=200; total time=   0.3s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=200; total time=   0.4s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=200; total time=   0.3s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=200; total time=   0.4s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=200; total time=   0.4s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=300; total time=   0.6s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=300; total time=   0.6s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=300; total time=   0.6s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=300; total time=   0.6s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=300; total time=   0.6s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=400; total time=   0.7s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=400; total time=   0.8s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=400; total time=   0.8s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=400; total time=   0.9s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=400; total time=   0.8s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=100; total time=   0.2s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=100; total time=   0.2s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=200; total time=   0.4s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=200; total time=   0.4s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=200; total time=   0.3s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=200; total time=   0.3s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=200; total time=   0.4s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=300; total time=   0.6s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=300; total time=   0.6s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=300; total time=   0.6s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=300; total time=   0.6s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=300; total time=   0.7s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=400; total time=   0.8s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=400; total time=   0.8s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=400; total time=   0.9s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=400; total time=   0.8s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=400; total time=   0.8s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=100; total time=   0.2s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=100; total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=100; total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=100; total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=100; total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=200; total time=   0.3s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=200; total time=   0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=200; total time=   0.4s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=200; total time=   0.3s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=200; total time=   0.3s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=300; total time=   0.6s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=300; total time=   0.6s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=300; total time=   0.6s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=300; total time=   0.5s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=300; total time=   0.6s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=400; total time=   0.8s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=400; total time=   0.8s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=400; total time=   0.8s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=400; total time=   0.8s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=400; total time=   0.8s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=100; total time=   0.2s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=100; total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=100; total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=100; total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=100; total time=   0.2s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=200; total time=   0.4s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=200; total time=   0.4s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=200; total time=   0.4s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=200; total time=   0.4s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=200; total time=   0.4s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=300; total time=   0.7s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=300; total time=   0.7s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=300; total time=   0.6s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=300; total time=   0.6s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=300; total time=   0.6s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=400; total time=   0.8s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=400; total time=   0.8s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=400; total time=   0.9s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=400; total time=   0.9s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=400; total time=   0.9s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=100; total time=   0.2s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=100; total time=   0.2s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=100; total time=   0.2s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=200; total time=   0.4s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=200; total time=   0.4s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=200; total time=   0.4s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=200; total time=   0.4s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=200; total time=   0.4s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=300; total time=   0.6s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=300; total time=   0.6s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=300; total time=   0.6s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=300; total time=   0.6s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=300; total time=   0.6s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=400; total time=   0.8s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=400; total time=   0.9s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=400; total time=   0.8s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=400; total time=   0.8s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=400; total time=   0.8s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=100; total time=   0.2s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=100; total time=   0.3s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=100; total time=   0.2s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=100; total time=   0.2s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=100; total time=   0.2s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=200; total time=   0.5s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=200; total time=   0.4s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=200; total time=   0.5s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=200; total time=   0.3s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=200; total time=   0.4s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=300; total time=   0.6s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=300; total time=   0.7s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=300; total time=   0.7s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=300; total time=   0.7s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=300; total time=   0.6s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=400; total time=   0.8s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=400; total time=   0.8s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=400; total time=   0.8s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=400; total time=   0.9s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=400; total time=   0.9s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=100; total time=   0.2s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=100; total time=   0.2s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=100; total time=   0.2s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=100; total time=   0.2s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=100; total time=   0.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=200; total time=   0.4s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=200; total time=   0.4s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=200; total time=   0.4s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=200; total time=   0.4s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=200; total time=   0.4s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=300; total time=   0.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=300; total time=   0.6s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=300; total time=   0.6s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=300; total time=   0.6s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=300; total time=   0.7s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=400; total time=   0.8s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=400; total time=   0.9s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=400; total time=   0.8s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=400; total time=   0.9s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=400; total time=   0.9s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=100; total time=   0.2s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=100; total time=   0.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=100; total time=   0.2s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=200; total time=   0.4s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=200; total time=   0.4s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=200; total time=   0.4s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=200; total time=   0.4s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=200; total time=   0.4s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=300; total time=   0.6s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=300; total time=   0.7s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=300; total time=   0.6s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=300; total time=   0.6s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=300; total time=   0.6s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=400; total time=   0.9s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=400; total time=   1.2s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=400; total time=   1.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=400; total time=   0.9s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=400; total time=   1.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=100; total time=   0.2s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=100; total time=   0.2s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=100; total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=100; total time=   0.2s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=100; total time=   0.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=200; total time=   0.4s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=200; total time=   0.5s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=200; total time=   0.4s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=200; total time=   0.4s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=200; total time=   0.4s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=300; total time=   0.6s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=300; total time=   0.6s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=300; total time=   0.6s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=300; total time=   0.7s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=300; total time=   0.6s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=400; total time=   0.9s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=400; total time=   0.9s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=400; total time=   1.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=400; total time=   1.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=400; total time=   1.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=100; total time=   0.2s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=100; total time=   0.2s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=100; total time=   0.3s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=100; total time=   0.3s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=100; total time=   0.3s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=200; total time=   0.6s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=200; total time=   0.5s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=200; total time=   0.6s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=200; total time=   0.5s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=200; total time=   0.6s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=300; total time=   0.8s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=300; total time=   0.7s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=300; total time=   0.7s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=300; total time=   0.7s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=300; total time=   0.7s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=400; total time=   0.9s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=400; total time=   1.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=400; total time=   1.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=400; total time=   1.7s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=400; total time=   1.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=100; total time=   0.2s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=100; total time=   0.2s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=100; total time=   0.2s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=100; total time=   0.2s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=100; total time=   0.2s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=200; total time=   0.4s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=200; total time=   0.5s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=200; total time=   0.5s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=200; total time=   0.5s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=200; total time=   0.5s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=300; total time=   0.7s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=300; total time=   0.7s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=300; total time=   0.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=300; total time=   0.7s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=300; total time=   0.7s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=400; total time=   1.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=400; total time=   0.9s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=400; total time=   1.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=400; total time=   1.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=400; total time=   1.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=100; total time=   0.2s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=100; total time=   0.2s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=100; total time=   0.2s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=100; total time=   0.2s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=100; total time=   0.2s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=200; total time=   0.5s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=200; total time=   0.5s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=200; total time=   0.4s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=200; total time=   0.5s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=200; total time=   0.4s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=300; total time=   0.7s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=300; total time=   0.8s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=300; total time=   0.8s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=300; total time=   0.7s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=300; total time=   0.7s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=400; total time=   1.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=400; total time=   1.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=400; total time=   1.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=400; total time=   1.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=400; total time=   1.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=100; total time=   0.2s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=100; total time=   0.2s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=100; total time=   0.2s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=100; total time=   0.2s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=100; total time=   0.2s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=200; total time=   0.5s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=200; total time=   0.5s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=200; total time=   0.5s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=200; total time=   0.4s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=200; total time=   0.5s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=300; total time=   0.8s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=300; total time=   0.8s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=300; total time=   0.8s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=300; total time=   0.8s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=300; total time=   0.8s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=400; total time=   1.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=400; total time=   1.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=400; total time=   1.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=400; total time=   1.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=400; total time=   1.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=100; total time=   0.2s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=100; total time=   0.2s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=100; total time=   0.2s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=100; total time=   0.2s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=100; total time=   0.2s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=200; total time=   0.5s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=200; total time=   0.5s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=200; total time=   0.5s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=200; total time=   0.5s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=200; total time=   0.5s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=300; total time=   0.7s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=300; total time=   0.7s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=300; total time=   0.7s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=300; total time=   0.7s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=300; total time=   0.8s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=400; total time=   1.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=400; total time=   1.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=400; total time=   1.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=400; total time=   1.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=400; total time=   1.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=100; total time=   0.2s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=100; total time=   0.3s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=100; total time=   0.2s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=100; total time=   0.2s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=100; total time=   0.2s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=200; total time=   0.5s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=200; total time=   0.5s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=200; total time=   0.5s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=200; total time=   0.5s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=200; total time=   0.5s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=300; total time=   0.7s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=300; total time=   0.8s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=300; total time=   0.8s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=300; total time=   0.8s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=300; total time=   0.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=400; total time=   1.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=400; total time=   1.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=400; total time=   1.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=400; total time=   1.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=400; total time=   1.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=100; total time=   0.2s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=100; total time=   0.2s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=100; total time=   0.2s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=100; total time=   0.2s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=100; total time=   0.2s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=200; total time=   0.5s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=200; total time=   0.5s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=200; total time=   0.5s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=200; total time=   0.5s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=200; total time=   0.5s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=300; total time=   0.8s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=300; total time=   0.8s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=300; total time=   0.8s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=300; total time=   0.9s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=300; total time=   0.8s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=400; total time=   1.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=400; total time=   1.2s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=400; total time=   1.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=400; total time=   1.2s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=400; total time=   1.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=100; total time=   0.2s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=100; total time=   0.3s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=100; total time=   0.3s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=100; total time=   0.4s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=100; total time=   0.2s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=200; total time=   0.6s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=200; total time=   0.5s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=200; total time=   0.6s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=200; total time=   0.6s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=200; total time=   0.5s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=300; total time=   0.8s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=300; total time=   0.9s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=300; total time=   0.8s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=300; total time=   0.8s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=300; total time=   0.9s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=400; total time=   1.2s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=400; total time=   1.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=400; total time=   1.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=400; total time=   1.2s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=400; total time=   1.2s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=100; total time=   0.2s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=100; total time=   0.2s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=100; total time=   0.2s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=100; total time=   0.2s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=100; total time=   0.2s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=200; total time=   0.5s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=200; total time=   0.5s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=200; total time=   0.5s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=200; total time=   0.5s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=200; total time=   0.5s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=300; total time=   0.8s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=300; total time=   0.9s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=300; total time=   0.8s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=300; total time=   0.8s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=300; total time=   0.8s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=400; total time=   1.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=400; total time=   1.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=400; total time=   1.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=400; total time=   1.2s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=400; total time=   1.2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=RandomForestClassifier(),\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': [2, 3, 4, 5],\n",
       "                         'max_features': ['auto', 'sqrt', 'log2'],\n",
       "                         'n_estimators': [100, 200, 300, 400]},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vOuJQodQ9TQ_",
    "outputId": "b0ac6eff-5507-4cc6-dad2-58682b0f488f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini',\n",
       " 'max_depth': 5,\n",
       " 'max_features': 'auto',\n",
       " 'n_estimators': 200}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "UxJe0ZJl54ks"
   },
   "outputs": [],
   "source": [
    "model10=grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "rJD7iURh57Es"
   },
   "outputs": [],
   "source": [
    "y_pred10= model10.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FtUgSp_I59BM",
    "outputId": "fd85e220-8095-463d-f155-eec030b8bada"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujmjRNA39eM_"
   },
   "source": [
    "# Extra Trees Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_fkVN9t9k_O"
   },
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "t_y9TuxK9nKl"
   },
   "outputs": [],
   "source": [
    "extra_tree_forest = ExtraTreesClassifier(n_estimators = 5,\n",
    "                                        criterion ='entropy', max_features = 2)\n",
    "extra_tree_forest.fit(x_train, y_train)\n",
    "feature_importance = extra_tree_forest.feature_importances_\n",
    "feature_importance_normalized = np.std([tree.feature_importances_ for tree in \n",
    "                                        extra_tree_forest.estimators_],\n",
    "                                        axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model11=extra_tree_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "m5O6hPgr9so3"
   },
   "outputs": [],
   "source": [
    "y_pred11=model11.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mfk-p97c9uuQ",
    "outputId": "ea7a8989-ada9-4c3a-a0fc-8f210fb0a833"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6rfbejR92cJ"
   },
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "uKzvquLg93pO"
   },
   "outputs": [],
   "source": [
    "param_grid={'n_estimators':[5,6,7,8,9],'criterion':['gini', 'entropy'],'max_depth':[2,3,4,5],'max_features':['auto','sqrt','log2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "SftU8pZg9_SI"
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(ExtraTreesClassifier(),param_grid,refit=True,verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uepIPi3G-BNR",
    "outputId": "c9be6753-9eb9-40a7-dd39-0f9c6e4b0854"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=2, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=8; total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=3, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=7; total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=5; total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=5, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=7; total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=2, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=3, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=5; total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=5; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=6; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=7; total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=7; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=8; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=auto, n_estimators=9; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=5; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=6; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=7; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=8; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=sqrt, n_estimators=9; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=5; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=6; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=7; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=8; total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=9; total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=5, max_features=log2, n_estimators=9; total time=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=ExtraTreesClassifier(),\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': [2, 3, 4, 5],\n",
       "                         'max_features': ['auto', 'sqrt', 'log2'],\n",
       "                         'n_estimators': [5, 6, 7, 8, 9]},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yEXyo9R_-DdB",
    "outputId": "8d1e1b1d-2d77-42cb-e05b-d3604d5e5e71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini',\n",
       " 'max_depth': 5,\n",
       " 'max_features': 'log2',\n",
       " 'n_estimators': 9}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "UxJe0ZJl54ks"
   },
   "outputs": [],
   "source": [
    "model12=grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "rJD7iURh57Es"
   },
   "outputs": [],
   "source": [
    "y_pred12= model12.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FtUgSp_I59BM",
    "outputId": "fd85e220-8095-463d-f155-eec030b8bada"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\t\t\t\t\t\t\t\t\t\t\tAccuracy\n",
      "\n",
      "XGB Classifier model 1\t\t\t\t\t\t\t\t\t 0.74\n",
      "XGB Classifier model 2\t\t\t\t\t\t\t\t\t 0.74\n",
      "AdaBoostClassifier(learning_rate=1) \t\t\t\t\t\t\t 0.39\n",
      "AdaBoostClassifier(learning_rate=2) \t\t\t\t\t\t\t 0.66\n",
      "GradientBoostingClassifier(learning_rate=1.0, max_depth=1) \t\t\t\t 0.65\n",
      "GradientBoostingClassifier(criterion='mse', learning_rate=1,\n",
      "                           max_features='auto') \t\t\t\t\t 0.61\n",
      "BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=500,\n",
      "                  random_state=10) \t\t\t\t\t\t\t 0.77\n",
      "BaggingClassifier(base_estimator=SVC(), n_estimators=500, random_state=9) \t\t 0.69\n",
      "RandomForestClassifier(max_depth=2, random_state=0) \t\t\t\t\t 0.68\n",
      "RandomForestClassifier(max_depth=5, n_estimators=200) \t\t\t\t\t 0.77\n",
      "ExtraTreesClassifier(criterion='entropy', max_features=2, n_estimators=5) \t\t 0.64\n",
      "ExtraTreesClassifier(max_depth=5, max_features='log2', n_estimators=9) \t\t\t 0.7\n"
     ]
    }
   ],
   "source": [
    "print('Model\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tAccuracy\\n')\n",
    "print('XGB Classifier model 1\\t\\t\\t\\t\\t\\t\\t\\t\\t',accuracy_score(y_test,y_pred1))\n",
    "print('XGB Classifier model 2\\t\\t\\t\\t\\t\\t\\t\\t\\t',accuracy_score(y_test,y_pred2))\n",
    "print(model3,'\\t\\t\\t\\t\\t\\t\\t',accuracy_score(y_test,y_pred3))\n",
    "print(model4,'\\t\\t\\t\\t\\t\\t\\t',accuracy_score(y_test,y_pred4))\n",
    "print(model5,'\\t\\t\\t\\t',accuracy_score(y_test,y_pred5))\n",
    "print(model6,'\\t\\t\\t\\t\\t',accuracy_score(y_test,y_pred6))\n",
    "print(model7,'\\t\\t\\t\\t\\t\\t\\t',accuracy_score(y_test,y_pred7))\n",
    "print(model8,'\\t\\t',accuracy_score(y_test,y_pred8))\n",
    "print(model9,'\\t\\t\\t\\t\\t',accuracy_score(y_test,y_pred9))\n",
    "print(model10,'\\t\\t\\t\\t\\t',accuracy_score(y_test,y_pred10))\n",
    "print(model11,'\\t\\t',accuracy_score(y_test,y_pred11))\n",
    "print(model12,'\\t\\t\\t',accuracy_score(y_test,y_pred12))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference : Thus for the given dataset BaggingClassifier - Model 1 and RandomForestClassifier - Model 2 give highest accuracies of 77%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CSE1015_Ensembling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
